{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9657868d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiomicTransformer:\n\tsize mismatch for tg_emb_table.weight: copying a param with shape torch.Size([25120, 384]) from checkpoint, the shape in current model is torch.Size([1425, 384]).\n\tsize mismatch for tg_decoder_table.weight: copying a param with shape torch.Size([25120, 384]) from checkpoint, the shape in current model is torch.Size([1425, 384]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 193\u001b[0m\n\u001b[1;32m    186\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiomicTransformer(\n\u001b[1;32m    187\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mD_MODEL, num_heads\u001b[38;5;241m=\u001b[39mNUM_HEADS, num_layers\u001b[38;5;241m=\u001b[39mNUM_LAYERS,\n\u001b[1;32m    188\u001b[0m     d_ff\u001b[38;5;241m=\u001b[39mD_FF, dropout\u001b[38;5;241m=\u001b[39mDROPOUT,\n\u001b[1;32m    189\u001b[0m     tf_vocab_size\u001b[38;5;241m=\u001b[39mtf_vocab_size, tg_vocab_size\u001b[38;5;241m=\u001b[39mtg_vocab_size,\n\u001b[1;32m    190\u001b[0m     use_shortcut\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    191\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    192\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 193\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# 5) forward pass on ALL eval cells (z-scored TFs)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m preds, true \u001b[38;5;241m=\u001b[39m run_model(model, test_loader, device, zscore_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.9/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiomicTransformer:\n\tsize mismatch for tg_emb_table.weight: copying a param with shape torch.Size([25120, 384]) from checkpoint, the shape in current model is torch.Size([1425, 384]).\n\tsize mismatch for tg_decoder_table.weight: copying a param with shape torch.Size([25120, 384]) from checkpoint, the shape in current model is torch.Size([1425, 384])."
     ]
    }
   ],
   "source": [
    "import os, sys, json, pickle, joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# ---- paths/config ------------------------------------------------------------\n",
    "PROJECT_DIR = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\"\n",
    "DEV_DIR     = os.path.join(PROJECT_DIR, \"dev/transformer\")\n",
    "sys.path.append(DEV_DIR)\n",
    "\n",
    "from transformer import MultiomicTransformer\n",
    "from transformer_dataset import MultiomicTransformerDataset\n",
    "from transformer_training import prepare_dataloader\n",
    "\n",
    "TRAINED_MODEL_SAMPLE_NAME = \"mESC\"\n",
    "EVAL_SAMPLE_NAME          = \"mESC_holdout\"\n",
    "CHROM_ID                  = \"chr1\"\n",
    "\n",
    "OUTPUT_DIR                = os.path.join(PROJECT_DIR, \"output/transformer_testing_output\")\n",
    "TRAINED_MODEL_DIR         = os.path.join(OUTPUT_DIR, \"mESC/chr1/model_training_29_09_17_51_50/iter1\")\n",
    "COMMON_DATA_DIR           = os.path.join(DEV_DIR, \"transformer_data\", \"common\")\n",
    "\n",
    "TRAINED_MODEL_DATASET_DIR = os.path.join(DEV_DIR, f\"transformer_data/{TRAINED_MODEL_SAMPLE_NAME}\")\n",
    "EVAL_DATASET_DIR          = os.path.join(DEV_DIR, f\"transformer_data/{EVAL_SAMPLE_NAME}\")\n",
    "\n",
    "CAL_SPLIT_FRAC = 0.5   # fraction of eval cells for fitting calibrator (rest is test)\n",
    "CAL_ALPHAS     = [0.1, 0.3, 1.0, 3.0, 10.0]\n",
    "BATCH_SIZE_FALLBACK = 64  # used if run_params missing or too small\n",
    "SEED = 42\n",
    "\n",
    "# ---- helpers ----------------------------------------------------------------\n",
    "def set_seed(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def inv_tr(X, mean, scale):\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    if scale is not None: X = X * scale\n",
    "    if mean  is not None: X = X + mean\n",
    "    return X\n",
    "\n",
    "def run_model(model, loader, device, zscore_tf=True):\n",
    "    \"\"\"Return preds/true in the dataset's z-space (preds are in TRAIN z-space by construction).\"\"\"\n",
    "    preds_all, true_all = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for atac_wins, tf_tensor, tg_true, bias, tf_ids, tg_ids in loader:\n",
    "            atac_wins = atac_wins.to(device)\n",
    "            tf_tensor = tf_tensor.to(device)\n",
    "            tg_true   = tg_true.to(device)\n",
    "            bias      = bias.to(device)\n",
    "            tf_ids    = tf_ids.to(device)\n",
    "            tg_ids    = tg_ids.to(device)\n",
    "\n",
    "            if zscore_tf:\n",
    "                mu = tf_tensor.mean(dim=1, keepdim=True)\n",
    "                sd = tf_tensor.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "                tf_tensor = (tf_tensor - mu) / sd\n",
    "\n",
    "            preds = model(atac_wins, tf_tensor, tf_ids=tf_ids, tg_ids=tg_ids, bias=bias)  # [B,G_eval]\n",
    "            preds_all.append(preds.cpu().numpy())\n",
    "            true_all.append(tg_true.cpu().numpy())\n",
    "    return np.vstack(preds_all), np.vstack(true_all)\n",
    "\n",
    "def build_overlap_and_spaces(preds, true, dataset, train_dataset_dir):\n",
    "    \"\"\"Align gene order, inverse-transform to raw spaces, and map truth to TRAIN z-space.\"\"\"\n",
    "    train_scaler = joblib.load(Path(train_dataset_dir) / f\"{CHROM_ID}/tg_scaler_{CHROM_ID}.pkl\")\n",
    "    eval_scaler  = dataset.scaler\n",
    "\n",
    "    with open(Path(train_dataset_dir) / f\"{CHROM_ID}/tg_names_{CHROM_ID}.json\") as f:\n",
    "        train_tg_names = json.load(f)\n",
    "\n",
    "    train_name_to_idx = {g:i for i,g in enumerate(train_tg_names)}\n",
    "    eval_name_to_idx  = {g:i for i,g in enumerate(dataset.tg_names)}\n",
    "\n",
    "    overlap_genes = [g for g in dataset.tg_names if g in train_name_to_idx]\n",
    "    mask_eval = np.array([g in train_name_to_idx for g in dataset.tg_names], dtype=bool)\n",
    "    train_idx = np.array([train_name_to_idx[g] for g in overlap_genes])\n",
    "    eval_idx  = np.array([eval_name_to_idx[g]  for g in overlap_genes])\n",
    "\n",
    "    # slice to overlap\n",
    "    preds_ov = preds[:, mask_eval]   # TRAIN z-space\n",
    "    true_ov  = true[:,  mask_eval]   # EVAL  z-space\n",
    "\n",
    "    # to raw\n",
    "    preds_raw = inv_tr(preds_ov, train_scaler.mean_[train_idx], train_scaler.scale_[train_idx])\n",
    "    true_raw  = inv_tr(true_ov,  eval_scaler.mean_[eval_idx],   eval_scaler.scale_[eval_idx])\n",
    "\n",
    "    # truth in TRAIN z-space (apples-to-apples w/ preds_ov)\n",
    "    true_in_train_z = (true_raw - train_scaler.mean_[train_idx]) / train_scaler.scale_[train_idx]\n",
    "\n",
    "    return {\n",
    "        \"overlap_genes\": overlap_genes,\n",
    "        \"preds_ov\": preds_ov,\n",
    "        \"true_ov\": true_ov,\n",
    "        \"preds_raw\": preds_raw,\n",
    "        \"true_raw\": true_raw,\n",
    "        \"true_in_train_z\": true_in_train_z,\n",
    "        \"train_scaler\": train_scaler,\n",
    "        \"train_idx\": train_idx\n",
    "    }\n",
    "\n",
    "def split_for_calibration(X, Y, frac=CAL_SPLIT_FRAC, seed=SEED):\n",
    "    \"\"\"Split rows into (calibration, evaluation) to avoid leakage.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idx = rng.permutation(n)\n",
    "    k  = int(np.floor(frac * n))\n",
    "    cal, test = idx[:k], idx[k:]\n",
    "    return (X[cal], Y[cal]), (X[test], Y[test])\n",
    "\n",
    "def fit_ridge_calibrator(X_cal, Y_cal, alphas=CAL_ALPHAS):\n",
    "    \"\"\"Multi-output Ridge with simple CV on the calibration split.\"\"\"\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import KFold\n",
    "    best_alpha, best_r = None, -np.inf\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for a in alphas:\n",
    "        rs = []\n",
    "        for tr, va in kf.split(X_cal):\n",
    "            rr = Ridge(alpha=a, fit_intercept=True)\n",
    "            rr.fit(X_cal[tr], Y_cal[tr])\n",
    "            yhat = rr.predict(X_cal[va])\n",
    "            rs.append(np.corrcoef(Y_cal[va].ravel(), yhat.ravel())[0,1])\n",
    "        r_mean = float(np.mean(rs))\n",
    "        if r_mean > best_r: best_r, best_alpha = r_mean, a\n",
    "    ridge = Ridge(alpha=best_alpha, fit_intercept=True)\n",
    "    ridge.fit(X_cal, Y_cal)\n",
    "    return ridge, best_alpha, best_r\n",
    "\n",
    "def metrics_block(y, yhat, title=\"\"):\n",
    "    r_p = pearsonr(y.ravel(), yhat.ravel())[0]\n",
    "    r_s = spearmanr(y.ravel(), yhat.ravel()).correlation\n",
    "    mae = np.mean(np.abs(y - yhat))\n",
    "    return {\"pearson\": float(r_p), \"spearman\": float(r_s), \"mae\": float(mae), \"title\": title}\n",
    "\n",
    "def scatter_plot(y, yhat, title, out_png, max_points=5000):\n",
    "    n = min(max_points, y.shape[0])\n",
    "    idx = np.random.RandomState(SEED).choice(y.shape[0], n, replace=False)\n",
    "    plt.figure(figsize=(6.5,6.5))\n",
    "    plt.scatter(y[idx].ravel(), yhat[idx].ravel(), alpha=0.25, s=12)\n",
    "    lims = [min(y.min(), yhat.min()), max(y.max(), yhat.max())]\n",
    "    plt.plot(lims, lims, 'r--', linewidth=1)\n",
    "    rp = pearsonr(y.ravel(), yhat.ravel())[0]\n",
    "    plt.title(f\"{title}\\nPearson r = {rp:.2f}\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "# ---- main -------------------------------------------------------------------\n",
    "set_seed()\n",
    "\n",
    "# 1) load run params + model\n",
    "with open(os.path.join(TRAINED_MODEL_DIR, \"run_parameters.json\")) as f:\n",
    "    run_params = json.load(f)\n",
    "D_MODEL   = run_params[\"d_model\"]\n",
    "NUM_HEADS = run_params[\"Attention Heads\"]\n",
    "NUM_LAYERS= run_params[\"Model Layers\"]\n",
    "D_FF      = run_params[\"d_feedforward\"]\n",
    "DROPOUT   = run_params[\"Dropout\"]\n",
    "BATCH     = run_params.get(\"Batch Size\", BATCH_SIZE_FALLBACK)\n",
    "\n",
    "ckpt_path = os.path.join(TRAINED_MODEL_DIR, \"checkpoint.pt\")\n",
    "\n",
    "# 2) vocabs (common)\n",
    "with open(os.path.join(COMMON_DATA_DIR, \"tf_vocab.json\")) as f: tf_vocab = json.load(f)\n",
    "with open(os.path.join(COMMON_DATA_DIR, f\"tg_vocab_{CHROM_ID}.json\")) as f: tg_vocab = json.load(f)\n",
    "tf_vocab_size, tg_vocab_size = len(tf_vocab), len(tg_vocab)\n",
    "\n",
    "# 3) eval dataset + loaders\n",
    "dataset = MultiomicTransformerDataset(\n",
    "    data_dir=EVAL_DATASET_DIR,\n",
    "    chrom_id=CHROM_ID,\n",
    "    tf_vocab_path=os.path.join(COMMON_DATA_DIR, \"tf_vocab.json\"),\n",
    "    tg_vocab_path=os.path.join(COMMON_DATA_DIR, f\"tg_vocab_{CHROM_ID}.json\"),\n",
    ")\n",
    "train_loader, val_loader, test_loader = prepare_dataloader(dataset, batch_size=BATCH, world_size=1, rank=0)\n",
    "\n",
    "# 4) model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiomicTransformer(\n",
    "    d_model=D_MODEL, num_heads=NUM_HEADS, num_layers=NUM_LAYERS,\n",
    "    d_ff=D_FF, dropout=DROPOUT,\n",
    "    tf_vocab_size=tf_vocab_size, tg_vocab_size=tg_vocab_size,\n",
    "    use_shortcut=True\n",
    ").to(device)\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "# 5) forward pass on ALL eval cells (z-scored TFs)\n",
    "preds, true = run_model(model, test_loader, device, zscore_tf=True)\n",
    "\n",
    "# 6) overlap alignment + spaces\n",
    "spaces = build_overlap_and_spaces(preds, true, dataset, TRAINED_MODEL_DATASET_DIR)\n",
    "overlap_genes   = spaces[\"overlap_genes\"]\n",
    "preds_ov        = spaces[\"preds_ov\"]          # TRAIN z-space\n",
    "true_in_train_z = spaces[\"true_in_train_z\"]   # TRAIN z-space\n",
    "preds_raw       = spaces[\"preds_raw\"]         # raw DS011 after train inverse\n",
    "true_raw        = spaces[\"true_raw\"]\n",
    "train_scaler    = spaces[\"train_scaler\"]\n",
    "train_idx       = spaces[\"train_idx\"]\n",
    "\n",
    "# 7) baseline metrics (train z-space & raw)\n",
    "base_train = metrics_block(true_in_train_z, preds_ov, \"Pre-calibration (train z-space)\")\n",
    "base_raw   = metrics_block(true_raw,       preds_raw, \"Pre-calibration (raw DS011)\")\n",
    "\n",
    "# 8) calibration without leakage (split eval rows)\n",
    "(X_cal, Y_cal), (X_test, Y_test) = split_for_calibration(preds_ov, true_in_train_z, frac=CAL_SPLIT_FRAC, seed=SEED)\n",
    "ridge, best_alpha, cv_r = fit_ridge_calibrator(X_cal, Y_cal, alphas=CAL_ALPHAS)\n",
    "\n",
    "# apply to test split\n",
    "preds_test_cal = ridge.predict(X_test)  # still in TRAIN z-space\n",
    "cal_train = metrics_block(Y_test, preds_test_cal, \"Post-calibration (train z-space, test split)\")\n",
    "\n",
    "# also view in raw DS011\n",
    "preds_test_cal_raw = inv_tr(preds_test_cal, train_scaler.mean_[train_idx], train_scaler.scale_[train_idx])\n",
    "Y_test_raw         = inv_tr(Y_test,         train_scaler.mean_[train_idx], train_scaler.scale_[train_idx])\n",
    "cal_raw = metrics_block(Y_test_raw, preds_test_cal_raw, \"Post-calibration (raw DS011, test split)\")\n",
    "\n",
    "# 9) save artifacts/plots\n",
    "out_dir = os.path.join(OUTPUT_DIR, f\"infer_{EVAL_SAMPLE_NAME}_{CHROM_ID}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(ridge, os.path.join(out_dir, f\"ridge_calibrator_alpha{best_alpha}.pkl\"))\n",
    "pd.DataFrame({\n",
    "    \"metric\": [\"pearson\",\"spearman\",\"mae\"],\n",
    "    \"pre_train\": [base_train[\"pearson\"], base_train[\"spearman\"], base_train[\"mae\"]],\n",
    "    \"pre_raw\":   [base_raw[\"pearson\"],   base_raw[\"spearman\"],   base_raw[\"mae\"]],\n",
    "    \"post_train\":[cal_train[\"pearson\"],  cal_train[\"spearman\"],  cal_train[\"mae\"]],\n",
    "    \"post_raw\":  [cal_raw[\"pearson\"],    cal_raw[\"spearman\"],    cal_raw[\"mae\"]],\n",
    "}).to_csv(os.path.join(out_dir, \"metrics.csv\"), index=False)\n",
    "\n",
    "# plots\n",
    "scatter_plot(Y_test_raw, preds_test_cal_raw,\n",
    "             f\"{EVAL_SAMPLE_NAME} {CHROM_ID}: Predicted vs Actual (calibrated, test split)\",\n",
    "             os.path.join(out_dir, \"scatter_calibrated_test.png\"))\n",
    "\n",
    "# per-gene correlation on test split (raw)\n",
    "gene_corr = []\n",
    "for j, g in enumerate(overlap_genes):\n",
    "    y = Y_test_raw[:, j]; yhat = preds_test_cal_raw[:, j]\n",
    "    r = pearsonr(y, yhat)[0] if np.std(y) > 1e-8 else 0.0\n",
    "    gene_corr.append((g, r))\n",
    "pd.DataFrame(gene_corr, columns=[\"gene\",\"pearson\"]).sort_values(\"pearson\", ascending=False)\\\n",
    "  .to_csv(os.path.join(out_dir, \"per_gene_pearson_test.csv\"), index=False)\n",
    "\n",
    "# save matrices (test split, raw)\n",
    "pd.DataFrame(preds_test_cal_raw, columns=overlap_genes)\\\n",
    "  .to_csv(os.path.join(out_dir, \"predictions_calibrated_test.csv\"), index=False)\n",
    "pd.DataFrame(Y_test_raw, columns=overlap_genes)\\\n",
    "  .to_csv(os.path.join(out_dir, \"truth_raw_test.csv\"), index=False)\n",
    "\n",
    "print(\"\\n== Summary ==\")\n",
    "print(f\"Baseline train z-space r: {base_train['pearson']:.3f}\")\n",
    "print(f\"Baseline raw DS011 r:     {base_raw['pearson']:.3f}\")\n",
    "print(f\"Calibrated train z r:     {cal_train['pearson']:.3f}\")\n",
    "print(f\"Calibrated raw r:         {cal_raw['pearson']:.3f}  (alpha={best_alpha}, CV r={cv_r:.3f})\")\n",
    "print(f\"Artifacts in: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b16bffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF overlap: 262\n",
      "TG count (eval): 189\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median per-gene r: 0.045425191896724317\n"
     ]
    }
   ],
   "source": [
    "print(\"TF overlap:\", len([t for t in dataset.tf_names if t in tf_vocab]))\n",
    "print(\"TG count (eval):\", len(dataset.tg_names))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "per_gene = [pearsonr(true_raw[:,j], preds_raw[:,j])[0] if np.std(true_raw[:,j])>1e-8 else 0.0\n",
    "            for j in range(preds_raw.shape[1])]\n",
    "plt.hist(per_gene, bins=40); plt.title(\"Per-gene Pearson (DS011)\"); plt.show()\n",
    "print(\"Median per-gene r:\", np.median(per_gene))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbecb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson after ridge calibration: 0.271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "N = preds_ov.shape[0]\n",
    "idx = np.random.RandomState(0).randint(0,2,size=N).astype(bool)\n",
    "reg = Ridge(alpha=1.0).fit(preds_ov[idx], true_in_train_z[idx])\n",
    "preds_cal = reg.predict(preds_ov[~idx])\n",
    "r = np.corrcoef(preds_cal.ravel(), true_in_train_z[~idx].ravel())[0,1]\n",
    "print(\"Pearson after ridge calibration:\", round(r,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da45fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for p in model.tf_emb_table.parameters(): p.requires_grad=False\n",
    "for p in model.tg_emb_table.parameters(): p.requires_grad=False\n",
    "for p in model.tg_decoder_table.parameters(): p.requires_grad=False\n",
    "for p in model.encoder.parameters(): p.requires_grad=False\n",
    "\n",
    "trainable = list(model.out_dense.parameters())\n",
    "if hasattr(model, \"shortcut_scale\"):\n",
    "    trainable += [model.shortcut_scale]\n",
    "opt = torch.optim.Adam(trainable, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# quick loop (5–10 epochs) on DS011 train split with MSE on z-space targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587f14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 10 genes: [('Lgr6', np.float64(-0.22220699333166521), np.float64(1.7548341562169596)), ('Cadm3', np.float64(-0.22121843738933739), np.float64(0.5634886508677992)), ('Plxna2', np.float64(-0.20193127345428796), np.float64(0.6482968068468365)), ('Nmnat2', np.float64(-0.19767455202192438), np.float64(1.8375082048863023)), ('Nos1ap', np.float64(-0.18555702032206745), np.float64(1.0255259683838551)), ('Gulp1', np.float64(-0.1788826705706547), np.float64(1.7308196354079923)), ('Rgs20', np.float64(-0.17721879440007843), np.float64(1.6943327399907808)), ('Paqr8', np.float64(-0.16993688369158352), np.float64(0.8894929622950064)), ('Astn1', np.float64(-0.1644960022645811), np.float64(1.659518986525713)), ('Gm16070', np.float64(-0.16387266733828978), np.float64(0.8031445683986057))]\n"
     ]
    }
   ],
   "source": [
    "sd_ratio = np.std(preds_raw,0) / np.clip(np.std(true_raw,0), 1e-6, None)\n",
    "bad = [(g, r, s) for g,r,s in zip(dataset.tg_names, per_gene, sd_ratio)]\n",
    "bad.sort(key=lambda x: x[1])  # lowest r first\n",
    "print(\"Worst 10 genes:\", bad[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2418c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# X = model outputs in TRAIN z-space (preds_ov)\n",
    "# Y = DS011 truth transformed into TRAIN z-space (true_in_train_z)\n",
    "\n",
    "N = preds_ov.shape[0]\n",
    "rng = np.random.default_rng(0)\n",
    "mask = rng.random(N) < 0.5        # 50/50 split\n",
    "\n",
    "ridge = Ridge(alpha=1.0, fit_intercept=True)\n",
    "ridge.fit(preds_ov[mask], true_in_train_z[mask])\n",
    "\n",
    "# Save for later use\n",
    "joblib.dump(ridge, os.path.join(out_dir, \"ridge_calibrator.pkl\"))\n",
    "\n",
    "# Use it\n",
    "preds_cal = ridge.predict(preds_ov)             # still in TRAIN z-space\n",
    "# If you want raw-space values aligned to DS011 truth:\n",
    "preds_cal_raw = preds_cal * train_scaler.scale_[train_idx] + train_scaler.mean_[train_idx]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
