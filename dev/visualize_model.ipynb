{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f382d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Tuple, Optional, List\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "PROJECT_DIR = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\"\n",
    "SRC_DIR = str(Path(PROJECT_DIR) / \"src\")\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "from multiomic_transformer.models.model import MultiomicTransformer\n",
    "from multiomic_transformer.datasets.dataset_refactor import MultiChromosomeDataset, SimpleScaler, fit_simple_scalers\n",
    "import multiomic_transformer.utils.experiment_loader as experiment_loader\n",
    "\n",
    "def tanh_scaled_figsize(\n",
    "    rows: int,\n",
    "    cols: int,\n",
    "    short_in: float = 3.0,\n",
    "    max_ratio: float = 5.0,\n",
    "    alpha: float = 0.85,\n",
    "    min_w: float = 2.5,\n",
    "    max_w: float = 10.0,\n",
    "    min_h: float = 2.0,\n",
    "    max_h: float = 10.0,\n",
    "):\n",
    "    if rows <= 0 or cols <= 0:\n",
    "        return (short_in, short_in)\n",
    "\n",
    "    raw_ratio = max(rows, cols) / max(1, min(rows, cols))  # >= 1\n",
    "    scaled_ratio = 1.0 + (max_ratio - 1.0) * math.tanh(alpha * math.log(raw_ratio))\n",
    "\n",
    "    if rows >= cols:\n",
    "        fig_w = short_in\n",
    "        fig_h = short_in * scaled_ratio\n",
    "    else:\n",
    "        fig_w = short_in * scaled_ratio\n",
    "        fig_h = short_in\n",
    "\n",
    "    # ---- NEW: clamp absolute size ----\n",
    "    fig_w = float(np.clip(fig_w, min_w, max_w))\n",
    "    fig_h = float(np.clip(fig_h, min_h, max_h))\n",
    "\n",
    "    return fig_w, fig_h\n",
    "\n",
    "\n",
    "def downsample_2d_mean(data: np.ndarray, max_rows=1500, max_cols=1500):\n",
    "    r, c = data.shape\n",
    "    row_bin = max(1, int(np.ceil(r / max_rows)))\n",
    "    col_bin = max(1, int(np.ceil(c / max_cols)))\n",
    "\n",
    "    r2 = (r // row_bin) * row_bin\n",
    "    c2 = (c // col_bin) * col_bin\n",
    "    data = data[:r2, :c2]\n",
    "\n",
    "    data = data.reshape(r2 // row_bin, row_bin, c2 // col_bin, col_bin).mean(axis=(1, 3))\n",
    "    return data, row_bin, col_bin\n",
    "\n",
    "def save_input_heatmaps_from_batch(exp, out_dir: Tuple[str, Path], batch_idx: int = 0):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    batch = next(iter(exp.test_loader))\n",
    "    atac_wins, tf_tensor, tg_expr_true, bias, tf_ids, tg_ids, motif_mask = batch\n",
    "\n",
    "    # ---- TF expression vector (usually [B, n_tf] or [B, n_tf, 1]) ----\n",
    "    tf0 = tf_tensor[batch_idx]\n",
    "    if tf0.ndim == 2 and tf0.shape[-1] == 1:\n",
    "        tf0 = tf0.squeeze(-1)\n",
    "\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        tf0,\n",
    "        out_dir / f\"input_tf_expr.svg\",\n",
    "        title=f\"Input TF Expr\",\n",
    "    )\n",
    "\n",
    "    # ---- TG true expression (often [B, n_tg] or [B, n_tg, 1]) ----\n",
    "    tg0 = tg_expr_true[batch_idx]\n",
    "    if tg0.ndim == 2 and tg0.shape[-1] == 1:\n",
    "        tg0 = tg0.squeeze(-1)\n",
    "\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        tg0,\n",
    "        out_dir / f\"input_tg_expr_true.svg\",\n",
    "        title=f\"Input TG Expr True\",\n",
    "    )\n",
    "    \n",
    "    # ---- Batch TG embedding -----\n",
    "    tg_ids0 = tg_ids.to(exp.model.tg_identity_emb.weight.device)\n",
    "\n",
    "    tg_id_slice = exp.model.tg_identity_emb.weight[tg_ids0]  # [193, 128]\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        tg_id_slice,\n",
    "        \"./dev/model_heatmaps_svg/tg_identity_emb_slice_batch.svg\",\n",
    "        title=\"TGs in Batch\",\n",
    "    )\n",
    "\n",
    "    # ---- Bias (shape varies; make it 2D-ish) ----\n",
    "    b0 = bias[batch_idx]\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        b0,\n",
    "        out_dir / f\"input_bias.svg\",\n",
    "        title=f\"Input Bias\",\n",
    "    )\n",
    "\n",
    "    # ---- Motif mask (often [B, n_tf, n_tg] or similar) ----\n",
    "    mm0 = motif_mask[batch_idx]\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        mm0,\n",
    "        out_dir / f\"input_motif_mask.svg\",\n",
    "        title=f\"Input Motif Mask\",\n",
    "    )\n",
    "\n",
    "    # ---- ATAC windows: pick something plottable ----\n",
    "    # If atac_wins is [B, n_peaks, n_bins] → save as 2D\n",
    "    # If higher-dim, flatten everything but last dim.\n",
    "    a0 = atac_wins[batch_idx]\n",
    "    if a0.ndim > 2:\n",
    "        a0 = a0.reshape(a0.shape[0], -1)  # keep first dim, flatten rest\n",
    "    save_heatmap_svg_rasterized_downsampled(\n",
    "        a0,\n",
    "        out_dir / f\"input_atac_wins.svg\",\n",
    "        title=f\"Input ATAC Wins\",\n",
    "        max_rows=1500,\n",
    "        max_cols=1500,\n",
    "    )\n",
    "\n",
    "    print(f\"Saved input heatmaps to: {out_dir}\")\n",
    "\n",
    "def save_heatmap_svg_rasterized_downsampled(\n",
    "    weight_tensor: torch.Tensor,\n",
    "    out_path: Tuple[str, Path],\n",
    "    title: str = \"\",\n",
    "    cmap: str = \"viridis\",\n",
    "    short_in: float = 3.0,\n",
    "    max_ratio: float = 5.0,\n",
    "    alpha: float = 0.85,\n",
    "    dpi: int = 200,\n",
    "    rasterize: bool = True,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    max_rows: int = 1500,   # <-- NEW\n",
    "    max_cols: int = 1500,   # <-- NEW\n",
    "    is_data: bool = False,\n",
    "):\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = weight_tensor.detach().cpu().numpy()\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data[None, :]  # shape (1, N)\n",
    "\n",
    "    if data.ndim != 2:\n",
    "        data = np.asarray(data).reshape(data.shape[0], -1)\n",
    "\n",
    "    rows, cols = data.shape\n",
    "    if rows > max_rows or cols > max_cols:\n",
    "        data, row_bin, col_bin = downsample_2d_mean(data, max_rows=max_rows, max_cols=max_cols)\n",
    "        rows, cols = data.shape\n",
    "        title = f\"{title}\"\n",
    "\n",
    "    fig_w, fig_h = tanh_scaled_figsize(\n",
    "        rows, cols,\n",
    "        short_in=short_in,\n",
    "        max_ratio=max_ratio,\n",
    "        alpha=alpha,\n",
    "        min_w=3.0, max_w=9.0,\n",
    "        min_h=2.0, max_h=7.0,\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    \n",
    "    if is_data:\n",
    "        cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    else:\n",
    "        cmap = cmap\n",
    "\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        cmap=cmap,\n",
    "        xticklabels=False,\n",
    "        yticklabels=False,\n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "\n",
    "    if rasterize and ax.collections:\n",
    "        ax.collections[0].set_rasterized(True)\n",
    "\n",
    "    title = title.replace(\"_\", \" \")\n",
    "    ax.set_title(title, fontsize=24)\n",
    "    ax.set_xlabel(f\"{cols}\", fontsize=32)\n",
    "    ax.set_ylabel(f\"{rows}\", fontsize=32)\n",
    "\n",
    "    fig.tight_layout(pad=0.15)\n",
    "    fig.savefig(out_path, format=out_path.suffix.lstrip(\".\"), dpi=dpi, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def iter_weight_matrices(model: torch.nn.Module) -> Iterator[Tuple[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Yields (name, weight_tensor) for 2D weight-like tensors.\n",
    "    Embeddings are treated as 2D weights too.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if not isinstance(param, torch.Tensor):\n",
    "            continue\n",
    "        if param.ndim == 2:\n",
    "            yield name, param.detach()\n",
    "        # If you ever want to include 1D params (bias, layernorm), handle separately.\n",
    "\n",
    "\n",
    "def split_in_proj_weight_qkv(\n",
    "    in_proj_weight: torch.Tensor,\n",
    "    d_model: Optional[int] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split PyTorch MultiheadAttention in_proj_weight into (Wq, Wk, Wv).\n",
    "    in_proj_weight shape: (3*d_model, d_model)\n",
    "    \"\"\"\n",
    "    if in_proj_weight.ndim != 2:\n",
    "        raise ValueError(f\"in_proj_weight must be 2D, got {in_proj_weight.shape}\")\n",
    "\n",
    "    rows, cols = in_proj_weight.shape\n",
    "    if d_model is None:\n",
    "        d_model = cols\n",
    "\n",
    "    if rows != 3 * d_model:\n",
    "        raise ValueError(\n",
    "            f\"Expected in_proj_weight shape (3*d_model, d_model) = ({3*d_model}, {d_model}), \"\n",
    "            f\"got {in_proj_weight.shape}\"\n",
    "        )\n",
    "\n",
    "    Wq = in_proj_weight[0:d_model, :]\n",
    "    Wk = in_proj_weight[d_model:2*d_model, :]\n",
    "    Wv = in_proj_weight[2*d_model:3*d_model, :]\n",
    "    return Wq, Wk, Wv\n",
    "\n",
    "\n",
    "def save_model_weight_heatmaps(\n",
    "    model: torch.nn.Module,\n",
    "    out_dir: Tuple[str, Path],\n",
    "    include: Optional[List[str]] = None,   # OR logic\n",
    "    exclude: Optional[List[str]] = None,   # drop if any match\n",
    "    cmap: str = \"viridis\",\n",
    "    max_rows: int = 1500,\n",
    "    max_cols: int = 1500,\n",
    "    dpi: int = 200,\n",
    "    short_in: float = 3.0,\n",
    "    max_ratio: float = 5.0,\n",
    "    alpha: float = 0.85,\n",
    "    split_qkv: bool = True,               # <-- NEW\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    saved = 0\n",
    "\n",
    "    for name, w in iter_weight_matrices(model):\n",
    "        # include/exclude filters\n",
    "        if include is not None and not any(s in name for s in include):\n",
    "            continue\n",
    "        if exclude is not None and any(s in name for s in exclude):\n",
    "            continue\n",
    "\n",
    "        # ---- Special case: split encoder self-attn packed QKV ----\n",
    "        # Typical names:\n",
    "        #   encoder.layers.0.self_attn.in_proj_weight\n",
    "        #   encoder.layers.1.self_attn.in_proj_weight\n",
    "        if split_qkv and name.endswith(\"self_attn.in_proj_weight\"):\n",
    "            d_model = w.shape[1]  # cols\n",
    "            Wq, Wk, Wv = split_in_proj_weight_qkv(w, d_model=d_model)\n",
    "\n",
    "            for tag, mat in [(\"Q\", Wq), (\"K\", Wk), (\"V\", Wv)]:\n",
    "                safe_name = name.replace(\"/\", \"_\") + f\"_{tag}\"\n",
    "                out_path = out_dir / f\"{safe_name}.svg\"\n",
    "                save_heatmap_svg_rasterized_downsampled(\n",
    "                    mat,\n",
    "                    out_path=out_path,\n",
    "                    title=f\"{name} [{tag}]\",\n",
    "                    cmap=cmap,\n",
    "                    short_in=short_in,\n",
    "                    max_ratio=max_ratio,\n",
    "                    alpha=alpha,\n",
    "                    dpi=dpi,\n",
    "                    # max_rows=max_rows,\n",
    "                    # max_cols=max_cols,\n",
    "                )\n",
    "                saved += 1\n",
    "\n",
    "            continue  # don't save the combined packed matrix unless you also want it\n",
    "        \n",
    "        else:\n",
    "            # ---- Default: save weight as-is ----\n",
    "            safe_name = name.replace(\"/\", \"_\")\n",
    "            out_path = out_dir / f\"{safe_name}.svg\"\n",
    "            save_heatmap_svg_rasterized_downsampled(\n",
    "                w,\n",
    "                out_path=out_path,\n",
    "                title=name,\n",
    "                cmap=cmap,\n",
    "                short_in=short_in,\n",
    "                max_ratio=max_ratio,\n",
    "                alpha=alpha,\n",
    "                dpi=dpi,\n",
    "                # max_rows=max_rows,\n",
    "                # max_cols=max_cols,\n",
    "            )\n",
    "        saved += 1\n",
    "\n",
    "    print(f\"Saved {saved} heatmaps to: {out_dir}\")\n",
    "\n",
    "def run_encoder_with_intermediates(encoder: torch.nn.TransformerEncoder,\n",
    "                                  x: torch.Tensor,\n",
    "                                  need_weights: bool = True):\n",
    "    \"\"\"\n",
    "    encoder: exp.model.encoder (TransformerEncoder)\n",
    "    x: [B, W, d_model]  (batch_first=True style)\n",
    "    Returns:\n",
    "      x_out: final [B, W, d_model]\n",
    "      enc_dict: OrderedDict of intermediate tensors\n",
    "    \"\"\"\n",
    "    enc_dict = OrderedDict()\n",
    "    h = x\n",
    "\n",
    "    for li, layer in enumerate(encoder.layers):\n",
    "        prefix = f\"encoder.layers.{li}\"\n",
    "\n",
    "        # ---- MultiheadAttention expects [L, N, E] unless batch_first=True inside MHA.\n",
    "        # In torch.nn.TransformerEncoderLayer, self_attn is MultiheadAttention(batch_first=...).\n",
    "        # We'll handle both cases robustly:\n",
    "        is_batch_first = getattr(layer.self_attn, \"batch_first\", False)\n",
    "\n",
    "        q = k = v = h\n",
    "        if not is_batch_first:\n",
    "            q = q.transpose(0, 1)  # [W, B, d]\n",
    "            k = k.transpose(0, 1)\n",
    "            v = v.transpose(0, 1)\n",
    "\n",
    "        # =========================\n",
    "        # Compute Q, K, V explicitly\n",
    "        # =========================\n",
    "        mha = layer.self_attn\n",
    "        d_model = mha.embed_dim\n",
    "        num_heads = mha.num_heads\n",
    "        head_dim = d_model // num_heads\n",
    "\n",
    "        # ---- Split packed projection weights ----\n",
    "        Wq, Wk, Wv = mha.in_proj_weight.chunk(3, dim=0)   # each [d_model, d_model]\n",
    "\n",
    "        if mha.in_proj_bias is not None:\n",
    "            bq, bk, bv = mha.in_proj_bias.chunk(3, dim=0)\n",
    "        else:\n",
    "            bq = bk = bv = None\n",
    "\n",
    "        # ---- Compute projected Q/K/V ----\n",
    "        # h is [B, W, d_model] (batch_first assumed here)\n",
    "        Q = torch.nn.functional.linear(h, Wq, bq)   # [B, W, d_model]\n",
    "        K = torch.nn.functional.linear(h, Wk, bk)\n",
    "        V = torch.nn.functional.linear(h, Wv, bv)\n",
    "\n",
    "        enc_dict[f\"{prefix}.self_attn.Q\"] = Q\n",
    "        enc_dict[f\"{prefix}.self_attn.K\"] = K\n",
    "        enc_dict[f\"{prefix}.self_attn.V\"] = V\n",
    "\n",
    "        # ---- Reshape into heads (optional but very useful) ----\n",
    "        B, W, _ = Q.shape\n",
    "        Qh = Q.view(B, W, num_heads, head_dim).transpose(1, 2)  # [B, H, W, d_head]\n",
    "        Kh = K.view(B, W, num_heads, head_dim).transpose(1, 2)\n",
    "        Vh = V.view(B, W, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        enc_dict[f\"{prefix}.self_attn.Q_heads\"] = Qh\n",
    "        enc_dict[f\"{prefix}.self_attn.K_heads\"] = Kh\n",
    "        enc_dict[f\"{prefix}.self_attn.V_heads\"] = Vh\n",
    "        \n",
    "        attn_out, attn_w = layer.self_attn(\n",
    "            q, k, v,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=False if need_weights else True,  # [B,H,W,W] if supported\n",
    "        )\n",
    "\n",
    "        if not is_batch_first:\n",
    "            attn_out = attn_out.transpose(0, 1)  # back to [B,W,d]\n",
    "\n",
    "        enc_dict[f\"{prefix}.self_attn.out\"] = attn_out\n",
    "        if need_weights and attn_w is not None:\n",
    "            # attn_w is typically [B, H, W, W] when average_attn_weights=False\n",
    "            enc_dict[f\"{prefix}.self_attn.weights\"] = attn_w\n",
    "\n",
    "        # ---- Residual 1 + Norm 1 ----\n",
    "        h_attn = h + layer.dropout1(attn_out)\n",
    "        enc_dict[f\"{prefix}.resid1\"] = h_attn\n",
    "        h_norm1 = layer.norm1(h_attn)\n",
    "        enc_dict[f\"{prefix}.norm1\"] = h_norm1\n",
    "\n",
    "        # ---- FFN ----\n",
    "        ffn1 = layer.linear1(h_norm1)\n",
    "        enc_dict[f\"{prefix}.ffn.linear1\"] = ffn1\n",
    "\n",
    "        # activation is usually GELU in newer torch, ReLU in older; handle both\n",
    "        if hasattr(layer, \"activation\"):\n",
    "            ffn_act = layer.activation(ffn1)\n",
    "        else:\n",
    "            # older versions used F.relu internally; safe fallback:\n",
    "            ffn_act = torch.relu(ffn1)\n",
    "        enc_dict[f\"{prefix}.ffn.act\"] = ffn_act\n",
    "\n",
    "        ffn_drop = layer.dropout(ffn_act)\n",
    "        enc_dict[f\"{prefix}.ffn.dropout\"] = ffn_drop\n",
    "\n",
    "        ffn2 = layer.linear2(ffn_drop)\n",
    "        enc_dict[f\"{prefix}.ffn.linear2\"] = ffn2\n",
    "\n",
    "        # ---- Residual 2 + Norm 2 ----\n",
    "        h_ffn = h_norm1 + layer.dropout2(ffn2)\n",
    "        enc_dict[f\"{prefix}.resid2\"] = h_ffn\n",
    "        h = layer.norm2(h_ffn)\n",
    "        enc_dict[f\"{prefix}.norm2\"] = h\n",
    "\n",
    "    return h, enc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9d591",
   "metadata": {},
   "source": [
    "### Load the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da856b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = experiment_loader.ExperimentLoader(\n",
    "    experiment_dir = \"/gpfs/Labs/Uzun/DATA/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/experiments/\",\n",
    "    experiment_name=\"mESC_E7.5_rep1_hvg_filter_disp_0.5\",\n",
    "    model_num=1,\n",
    ")\n",
    "\n",
    "exp.load_trained_model(\"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e928f1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiomicTransformer(\n",
       "  (tf_identity_emb): Embedding(256, 128)\n",
       "  (tg_query_emb): Embedding(25090, 128)\n",
       "  (tg_identity_emb): Embedding(25090, 128)\n",
       "  (tf_expr_dense_input_layer): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (atac_acc_dense_input_layer): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (posenc): PositionalEmbedding()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_tf_to_atac): CrossAttention(\n",
       "    (attn): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cross_atac_to_tf): CrossAttention(\n",
       "    (attn): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tf_to_atac_cross_attn_pool): AttentionPooling()\n",
       "  (atac_to_tf_cross_attn_pool): AttentionPooling()\n",
       "  (pooled_cross_attn_dense_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=False)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cross_tg_to_atac): CrossAttention(\n",
       "    (attn): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (gene_pred_dense): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbff250",
   "metadata": {},
   "source": [
    "### Save the Heatmaps of the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd39e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 41 heatmaps to: dev/model_heatmaps_svg/model_weights\n"
     ]
    }
   ],
   "source": [
    "save_model_weight_heatmaps(\n",
    "    exp.model,\n",
    "    \"./dev/model_heatmaps_svg/model_weights\",\n",
    "    split_qkv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213e884",
   "metadata": {},
   "source": [
    "### Save Heatmaps of the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae8a969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_heatmap_svg_rasterized_downsampled(\n",
    "    exp.model.tf_to_atac_cross_attn_pool.query,\n",
    "    \"./dev/model_heatmaps_svg/tf_to_atac_cross_attn_pool.query.svg\",\n",
    "    title=\"TF→ATAC Attention Pool Query\",\n",
    ")\n",
    "\n",
    "save_heatmap_svg_rasterized_downsampled(\n",
    "    exp.model.atac_to_tf_cross_attn_pool.query,\n",
    "    \"./dev/model_heatmaps_svg/atac_to_tf_cross_attn_pool.query.svg\",\n",
    "    title=\"ATAC→TF Attention Pool Query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54c17ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "\n",
    "batch = next(iter(exp.test_loader))\n",
    "atac_wins, tf_tensor, tg_expr_true, bias, tf_ids, tg_ids, motif_mask = batch\n",
    "device = next(exp.model.parameters()).device\n",
    "\n",
    "# move tensors you actually compute with\n",
    "atac_wins  = atac_wins.to(device)\n",
    "tf_tensor  = tf_tensor.to(device)\n",
    "tf_ids     = tf_ids.to(device)\n",
    "tg_ids     = tg_ids.to(device)\n",
    "bias       = None if bias is None else bias.to(device)\n",
    "motif_mask = None if motif_mask is None else motif_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # -------------------- ATAC dense input layer --------------------\n",
    "    atac_lin1        = exp.model.atac_acc_dense_input_layer[0](atac_wins)        # [B, W, d_ff]\n",
    "    atac_silu        = exp.model.atac_acc_dense_input_layer[1](atac_lin1)        # [B, W, d_ff]\n",
    "    atac_dropout     = exp.model.atac_acc_dense_input_layer[2](atac_silu)        # [B, W, d_ff]\n",
    "    atac_lin2        = exp.model.atac_acc_dense_input_layer[3](atac_dropout)     # [B, W, d_model]\n",
    "    atac_layer_norm  = exp.model.atac_acc_dense_input_layer[4](atac_lin2)        # [B, W, d_model]\n",
    "\n",
    "    win_emb = atac_layer_norm                                                   # [B, W, d_model]\n",
    "\n",
    "    # positional encoding + add\n",
    "    W = win_emb.shape[1]\n",
    "    pos = torch.arange(W, device=device, dtype=torch.float32)\n",
    "    pos_emb = exp.model.posenc(pos, bsz=win_emb.shape[0]).transpose(0, 1)        # [B, W, d_model]\n",
    "    win_emb_pos = win_emb + pos_emb                                             # [B, W, d_model]\n",
    "\n",
    "    # -------------------- ATAC Transformer encoder --------------------\n",
    "    win_enc, encoder_dict = run_encoder_with_intermediates(exp.model.encoder, win_emb_pos, need_weights=True)                                                 # [B, W, d_model]\n",
    "\n",
    "    # -------------------- TF embeddings --------------------\n",
    "    tf_id_emb = exp.model.tf_identity_emb(tf_ids)                                # [T, d_model]\n",
    "\n",
    "    # TF expr dense input layer (step-by-step)\n",
    "    tf_expr_in = tf_tensor.unsqueeze(-1)                                         # [B, T, 1]\n",
    "    tf_lin1    = exp.model.tf_expr_dense_input_layer[0](tf_expr_in)              # [B, T, d_ff]\n",
    "    tf_silu    = exp.model.tf_expr_dense_input_layer[1](tf_lin1)                 # [B, T, d_ff]\n",
    "    tf_dropout = exp.model.tf_expr_dense_input_layer[2](tf_silu)                 # [B, T, d_ff]\n",
    "    tf_lin2    = exp.model.tf_expr_dense_input_layer[3](tf_dropout)              # [B, T, d_model]\n",
    "    tf_ln      = exp.model.tf_expr_dense_input_layer[4](tf_lin2)                 # [B, T, d_model]\n",
    "    tf_expr_emb = tf_ln\n",
    "\n",
    "    # combined TF embedding used for cross-attn\n",
    "    tf_emb = tf_expr_emb + tf_id_emb.unsqueeze(0)                                # [B, T, d_model]\n",
    "\n",
    "    # -------------------- TF<->ATAC cross attention --------------------\n",
    "    # CrossAttention forward: out = norm(query + 0.1*dropout(attn(query, kv)))\n",
    "    tf_cross   = exp.model.cross_tf_to_atac(tf_emb,  win_enc)                    # [B, T, d_model]\n",
    "    atac_cross = exp.model.cross_atac_to_tf(win_enc, tf_emb)                     # [B, W, d_model]\n",
    "\n",
    "    # -------------------- Attention pooling --------------------\n",
    "    tf_repr,   tf_pool_weights   = exp.model.tf_to_atac_cross_attn_pool(tf_cross)     # [B, d_model], [B, T, 1]\n",
    "    atac_repr, atac_pool_weights = exp.model.atac_to_tf_cross_attn_pool(atac_cross)  # [B, d_model], [B, W, 1]\n",
    "\n",
    "    # -------------------- Pooled cross-attn dense layer --------------------\n",
    "    pooled_cat = torch.cat([tf_repr, atac_repr], dim=-1)                         # [B, 2*d_model]\n",
    "    pooled_lin1    = exp.model.pooled_cross_attn_dense_layer[0](pooled_cat)      # [B, d_ff]\n",
    "    pooled_gelu    = exp.model.pooled_cross_attn_dense_layer[1](pooled_lin1)     # [B, d_ff]\n",
    "    pooled_dropout = exp.model.pooled_cross_attn_dense_layer[2](pooled_gelu)     # [B, d_ff]\n",
    "    pooled_lin2    = exp.model.pooled_cross_attn_dense_layer[3](pooled_dropout)  # [B, d_model]\n",
    "    pooled_ln      = exp.model.pooled_cross_attn_dense_layer[4](pooled_lin2)     # [B, d_model]\n",
    "    tf_atac_cross_attn_output = pooled_ln                                        # [B, d_model]\n",
    "\n",
    "    # -------------------- TG query / identity embeddings --------------------\n",
    "    tg_query_emb = exp.model.tg_query_emb(tg_ids)                                # [G, d_model]\n",
    "    tg_base = tg_query_emb.unsqueeze(0).expand(win_enc.shape[0], -1, -1)         # [B, G, d_model]\n",
    "\n",
    "    tg_id_emb = exp.model.tg_identity_emb(tg_ids)                                # [G, d_model]\n",
    "\n",
    "    # -------------------- distance bias shaping (as in forward) --------------------\n",
    "    attn_bias = None\n",
    "    if exp.model.use_bias and (bias is not None):\n",
    "        attn_bias = bias\n",
    "        if attn_bias.dim() == 3:\n",
    "            attn_bias = attn_bias.unsqueeze(1)                                   # [B, 1, G, W]\n",
    "        if attn_bias.shape[1] == 1:\n",
    "            attn_bias = attn_bias.expand(win_enc.shape[0], exp.model.num_heads, tg_base.size(1), win_enc.size(1))\n",
    "        attn_bias = torch.nan_to_num(attn_bias, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        attn_bias = (exp.model.bias_scale * attn_bias).clamp_(-20.0, 20.0)       # [B, H, G, W]\n",
    "\n",
    "    # -------------------- TG->ATAC cross attention --------------------\n",
    "    tg_cross = exp.model.cross_tg_to_atac(tg_base, win_enc, attn_bias=attn_bias) # [B, G, d_model]\n",
    "\n",
    "    # add pooled TF/ATAC summary to each TG (scaled)\n",
    "    n_tgs = tg_cross.size(1)\n",
    "    scale = 1.0 / math.sqrt(max(1, n_tgs))\n",
    "    tf_atac_expand = tf_atac_cross_attn_output.unsqueeze(1).expand(-1, n_tgs, -1) * scale  # [B, G, d_model]\n",
    "    tg_cross_attn_repr = tg_cross + tf_atac_expand                                # [B, G, d_model]\n",
    "\n",
    "    # -------------------- TG identity dot + gene_pred_dense --------------------\n",
    "    tg_similarity_to_attn_output = (tg_cross_attn_repr * tg_id_emb.unsqueeze(0)).sum(dim=-1)  # [B, G]\n",
    "\n",
    "    gene_lin1    = exp.model.gene_pred_dense[0](tg_cross_attn_repr)               # [B, G, d_ff]\n",
    "    gene_relu    = exp.model.gene_pred_dense[1](gene_lin1)                        # [B, G, d_ff]\n",
    "    gene_dropout = exp.model.gene_pred_dense[2](gene_relu)                        # [B, G, d_ff]\n",
    "    gene_lin2    = exp.model.gene_pred_dense[3](gene_dropout)                     # [B, G, 1]\n",
    "    gene_pred_term = gene_lin2.squeeze(-1)                                        # [B, G]\n",
    "\n",
    "    tg_pred_pre_shortcut = tg_similarity_to_attn_output + gene_pred_term          # [B, G]\n",
    "\n",
    "    # -------------------- Optional TF->TG shortcut --------------------\n",
    "    shortcut_out = None\n",
    "    shortcut_attn = None\n",
    "    tg_pred = tg_pred_pre_shortcut\n",
    "\n",
    "    if getattr(exp.model, \"use_shortcut\", False) and hasattr(exp.model, \"shortcut_layer\"):\n",
    "        # shortcut_layer expects: tg_emb [G,d], tf_id_emb [T,d], tf_expr [B,T], motif_mask [G,T]\n",
    "        # If your motif_mask is batched, slice batch 0; otherwise pass as-is.\n",
    "        mm_for_shortcut = None\n",
    "        if motif_mask is not None:\n",
    "            mm_for_shortcut = motif_mask[0] if motif_mask.dim() == 3 else motif_mask\n",
    "\n",
    "        shortcut_out, shortcut_attn = exp.model.shortcut_layer(\n",
    "            tg_id_emb, tf_id_emb, tf_tensor, motif_mask=mm_for_shortcut\n",
    "        )  # shortcut_out: [B,G], shortcut_attn: [G,T]\n",
    "        tg_pred = tg_pred + shortcut_out                                          # [B, G]\n",
    "\n",
    "\n",
    "# -------------------- model_dict (grouped outputs) --------------------\n",
    "model_dict = OrderedDict({\n",
    "    \"inputs\": [\n",
    "        (\"atac_windows\", atac_wins),\n",
    "        (\"tf_expr\", tf_tensor),\n",
    "        (\"tg_expr_true\", tg_expr_true),\n",
    "        (\"bias\", bias),\n",
    "        (\"tf_ids\", tf_ids),\n",
    "        (\"tg_ids\", tg_ids),\n",
    "        (\"motif_mask\", motif_mask),\n",
    "    ],\n",
    "\n",
    "    \"atac_dense_input_layer\": [\n",
    "        (\"lin1\", atac_lin1),\n",
    "        (\"silu\", atac_silu),\n",
    "        (\"dropout\", atac_dropout),\n",
    "        (\"lin2\", atac_lin2),\n",
    "        (\"layer_norm\", atac_layer_norm),\n",
    "        (\"pos_emb\", pos_emb),\n",
    "        (\"win_emb_pos\", win_emb_pos),\n",
    "    ],\n",
    "\n",
    "    \"window_encoder\": (\n",
    "        [(\"output\", win_enc)]\n",
    "        + [(k, v) for k, v in encoder_dict.items()]\n",
    "    ),\n",
    "\n",
    "    \"tf_id_emb\": [\n",
    "        (\"embedding\", tf_id_emb),\n",
    "    ],\n",
    "\n",
    "    \"tf_dense_input_layer\": [\n",
    "        (\"expr_in\", tf_expr_in),\n",
    "        (\"lin1\", tf_lin1),\n",
    "        (\"silu\", tf_silu),\n",
    "        (\"dropout\", tf_dropout),\n",
    "        (\"lin2\", tf_lin2),\n",
    "        (\"layer_norm\", tf_ln),\n",
    "    ],\n",
    "\n",
    "    \"tf_emb_combined\": [\n",
    "        (\"tf_expr_emb\", tf_expr_emb),\n",
    "        (\"tf_emb\", tf_emb),\n",
    "    ],\n",
    "\n",
    "    \"cross_tf_to_atac\": [\n",
    "        (\"tf_cross\", tf_cross),\n",
    "    ],\n",
    "\n",
    "    \"cross_atac_to_tf\": [\n",
    "        (\"atac_cross\", atac_cross),\n",
    "    ],\n",
    "\n",
    "    \"attention_pooling\": [\n",
    "        (\"tf_repr\", tf_repr),\n",
    "        (\"tf_pool_weights\", tf_pool_weights),\n",
    "        (\"atac_repr\", atac_repr),\n",
    "        (\"atac_pool_weights\", atac_pool_weights),\n",
    "    ],\n",
    "\n",
    "    \"pooled_cross_attn_dense_layer\": [\n",
    "        (\"cat_tf_atac\", pooled_cat),\n",
    "        (\"lin1\", pooled_lin1),\n",
    "        (\"gelu\", pooled_gelu),\n",
    "        (\"dropout\", pooled_dropout),\n",
    "        (\"lin2\", pooled_lin2),\n",
    "        (\"layer_norm\", pooled_ln),\n",
    "        (\"tf_atac_cross_attn_output\", tf_atac_cross_attn_output),\n",
    "    ],\n",
    "\n",
    "    \"tg_embeddings\": [\n",
    "        (\"tg_query_emb\", tg_query_emb),\n",
    "        (\"tg_base\", tg_base),\n",
    "        (\"tg_id_emb\", tg_id_emb),\n",
    "    ],\n",
    "\n",
    "    \"tg_to_atac_bias\": [\n",
    "        (\"attn_bias\", attn_bias),\n",
    "    ],\n",
    "\n",
    "    \"cross_tg_to_atac\": [\n",
    "        (\"tg_cross\", tg_cross),\n",
    "    ],\n",
    "\n",
    "    \"tg_cross_attn_fusion\": [\n",
    "        (\"tf_atac_expand\", tf_atac_expand),\n",
    "        (\"tg_cross_attn_repr\", tg_cross_attn_repr),\n",
    "    ],\n",
    "\n",
    "    \"tg_prediction_head\": [\n",
    "        (\"tg_similarity_to_attn_output\", tg_similarity_to_attn_output),\n",
    "        (\"gene_lin1\", gene_lin1),\n",
    "        (\"gene_relu\", gene_relu),\n",
    "        (\"gene_dropout\", gene_dropout),\n",
    "        (\"gene_lin2\", gene_lin2),\n",
    "        (\"gene_pred_term\", gene_pred_term),\n",
    "        (\"tg_pred_pre_shortcut\", tg_pred_pre_shortcut),\n",
    "        (\"tg_pred_final\", tg_pred),\n",
    "    ],\n",
    "\n",
    "    \"tf_tg_shortcut\": [\n",
    "        (\"shortcut_out\", shortcut_out),\n",
    "        (\"shortcut_attn\", shortcut_attn),\n",
    "    ],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b61ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved grouped heatmaps to: dev/model_heatmaps_svg\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from collections.abc import Mapping\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "def _select_plottable(x: torch.Tensor, batch_idx: int = 0, head_idx: int = 0):\n",
    "    \"\"\"\n",
    "    Convert common model tensors to plottable 1D/2D by slicing batch/head.\n",
    "\n",
    "    Rules:\n",
    "    - 1D/2D: keep\n",
    "    - 3D: assume [B, *, *] -> x[batch_idx]  -> [*, *] or [*]\n",
    "    - 4D: assume [B, H, *, *] or [B, H, *, d] -> x[batch_idx, head_idx] -> [*, *]\n",
    "    - >4D: slice batch then flatten remaining dims to 2D.\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(x):\n",
    "        return x\n",
    "\n",
    "    if x.ndim <= 2:\n",
    "        return x\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        return x[batch_idx]\n",
    "\n",
    "    if x.ndim == 4:\n",
    "        return x[batch_idx, head_idx]\n",
    "\n",
    "    y = x[batch_idx]\n",
    "    if y.ndim > 2:\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "    return y\n",
    "\n",
    "def save_grouped_heatmaps(\n",
    "    model_dict,\n",
    "    out_root: Tuple[str, Path],\n",
    "    batch_idx: int = 0,\n",
    "    head_idx: int = 0,\n",
    "    dpi: int = 200,\n",
    "    cmap: str = \"viridis\",\n",
    "    skip_none: bool = True,\n",
    "):\n",
    "    out_root = Path(out_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_parts = len(model_dict)\n",
    "    part_digits = max(2, len(str(n_parts - 1)))\n",
    "\n",
    "    def _save_items(items, part_name: str, part_dir: Path):\n",
    "        # items is expected to be iterable of (name, obj)\n",
    "        n_items = len(items)\n",
    "        file_digits = max(2, len(str(n_items - 1)))\n",
    "        saved_i = 0\n",
    "\n",
    "        for item_name, obj in items:\n",
    "            if obj is None:\n",
    "                if skip_none:\n",
    "                    continue\n",
    "                raise ValueError(f\"{part_name}/{item_name} is None\")\n",
    "\n",
    "            # --- recurse if nested dict/OrderedDict ---\n",
    "            if isinstance(obj, Mapping):\n",
    "                sub_dir = part_dir / f\"{saved_i:0{file_digits}d}_{_slug(item_name)}\"\n",
    "                sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "                _save_items(list(obj.items()), f\"{part_name}/{item_name}\", sub_dir)\n",
    "                saved_i += 1\n",
    "                continue\n",
    "\n",
    "            # --- tensor case ---\n",
    "            if not torch.is_tensor(obj):\n",
    "                if skip_none:\n",
    "                    continue\n",
    "                raise TypeError(f\"{part_name}/{item_name} is not a tensor: {type(obj)}\")\n",
    "\n",
    "            x = _select_plottable(obj, batch_idx=batch_idx, head_idx=head_idx)\n",
    "\n",
    "            fname = f\"{saved_i:0{file_digits}d}_{_slug(item_name)}.svg\"\n",
    "            out_path = part_dir / fname\n",
    "            title = f\"{part_name}\\n{item_name}\"\n",
    "\n",
    "            save_heatmap_svg_rasterized_downsampled(\n",
    "                x,\n",
    "                out_path,\n",
    "                title=title,\n",
    "                cmap=cmap,\n",
    "                dpi=dpi,\n",
    "            )\n",
    "            saved_i += 1\n",
    "\n",
    "    for part_i, (part_name, items) in enumerate(model_dict.items()):\n",
    "        part_slug = _slug(part_name)\n",
    "        part_dir = out_root / f\"{part_i:0{part_digits}d}_{part_slug}\"\n",
    "        part_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # items should be a list of (name, tensor/obj)\n",
    "        _save_items(list(items), part_name, part_dir)\n",
    "\n",
    "    print(f\"Saved grouped heatmaps to: {out_root}\")\n",
    "\n",
    "\n",
    "\n",
    "save_grouped_heatmaps(\n",
    "    model_dict,\n",
    "    out_root=\"./dev/model_heatmaps_svg\",\n",
    "    batch_idx=0,\n",
    "    head_idx=0,\n",
    "    dpi=200,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
