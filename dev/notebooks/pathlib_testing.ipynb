{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf8504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/config/settings.py\n",
      "settings.py\n",
      "settings\n",
      ".py\n",
      "/\n",
      "/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/config\n",
      "/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/config/settings.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m---> 14\u001b[0m time, file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mfromtimestamp(time), file_path)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m---> 14\u001b[0m time, file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((f\u001b[38;5;241m.\u001b[39mstat()\u001b[38;5;241m.\u001b[39mst_mtime, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m path\u001b[38;5;241m.\u001b[39miterdir())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mfromtimestamp(time), file_path)\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.9/pathlib.py:1160\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1162\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/config/settings.py'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "path = Path(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/config/settings.py\")\n",
    "\n",
    "print(path)\n",
    "print(path.name)\n",
    "print(path.stem)\n",
    "print(path.suffix)\n",
    "print(path.anchor)\n",
    "print(path.parent)\n",
    "print(path.parent.parent.parent)\n",
    "\n",
    "time, file_path = max((f.stat().st_mtime, f) for f in path.iterdir())\n",
    "print(datetime.fromtimestamp(time), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/transformer_testing_output/mESC/chr19/model_training_001\n"
     ]
    }
   ],
   "source": [
    "def unique_path(directory: Path, name_pattern: str):\n",
    "    \"\"\"\n",
    "    Returns a path for the next iteration in a directory naming pattern\n",
    "    \n",
    "    e.g. if `directory` contains the subdirectories `test_001` and `test_002`,\n",
    "    then the template `test_{:03d}` will return `test_003`.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "        directory (Path)\n",
    "            A `Path` object for the target directory.\n",
    "        name_pattern (str)\n",
    "            An iterable pattern for the iteration.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        path (Path)\n",
    "        Path for the next iteration in the pattern.\n",
    "    \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        path = directory / name_pattern.format(counter)\n",
    "        if not path.exists():\n",
    "            return path\n",
    "\n",
    "template = \"model_training_{:03d}\"\n",
    "new_path = unique_path(path, template)\n",
    "print(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1798e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ /gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/transformer_testing_output/mESC/chr19\n",
      "    + genes_near_peaks.parquet\n",
      "    + model_training_01_10_10_03_31\n",
      "        + iter1\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter2\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter3\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter4\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter5\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter6\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter7\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + trained_model.pt\n",
      "            + training_log.csv\n",
      "        + iter8\n",
      "            + checkpoint.pt\n",
      "            + run_parameters.json\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + training_log.csv\n",
      "    + model_training_01_10_13_06_52\n",
      "    + model_training_01_10_13_12_48\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_13_17_20\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_13_22_07\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_14_52_54\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_15_15_36\n",
      "        + checkpoint.pt\n",
      "        + eval_results_pearson_corr.png\n",
      "        + eval_results_scatter.png\n",
      "        + eval_results_training_loss.png\n",
      "        + run_parameters.json\n",
      "        + tf_gradient_attributions\n",
      "            + scored_edges.tsv\n",
      "            + tf_importance_matrix.csv\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_15_15_45\n",
      "        + tf_gradient_attributions\n",
      "    + model_training_01_10_16_03_45\n",
      "        + checkpoint.pt\n",
      "        + eval_results_pearson_corr.png\n",
      "        + eval_results_scatter.png\n",
      "        + eval_results_training_loss.png\n",
      "        + run_parameters.json\n",
      "        + tf_gradient_attributions\n",
      "            + scored_edges.tsv\n",
      "            + tf_importance_matrix.csv\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_tg_shortcut_matrix.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_18_09_38\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_18_25_26\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_18_28_44\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_18_30_20\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_18_51_40\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_18_56_03\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_18_58_12\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_01_19\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_02_59\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_06_20\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_07_55\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_01_10_19_10_33\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_14_55\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_01_10_19_23_20\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_12_57_00\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_12_59_45\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_13_03_46\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_13_10_57\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_13_19_31\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_13_21_57\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_02_10_13_28_09\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_02_10_13_29_40\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_02_10_13_34_23\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "    + model_training_02_10_13_38_57\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_14_08_30\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_14_08_31\n",
      "        + checkpoint.pt\n",
      "        + eval_results_pearson_corr.png\n",
      "        + eval_results_scatter.png\n",
      "        + eval_results_training_loss.png\n",
      "        + run_parameters.json\n",
      "        + tf_gradient_attributions\n",
      "            + gradient_attribution.csv\n",
      "            + scored_edges.tsv\n",
      "            + shortcut_matrix.csv\n",
      "            + tf_importance_matrix.csv\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_14_50_47\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_14_56_14\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_15_09_33\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_15_15_22\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_15_30_50\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_tg_attention_heatmap.png\n",
      "        + tf_tg_mean_attention.csv\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_15_36_57\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_15_42_48\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_16_41_50\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_02_10_16_49_39\n",
      "        + checkpoint.pt\n",
      "        + run_parameters.json\n",
      "        + tf_vocab.json\n",
      "        + tg_scaler.pkl\n",
      "        + tg_vocab.json\n",
      "        + trained_model.pt\n",
      "        + training_log.csv\n",
      "    + model_training_30_09_17_33_32\n",
      "        + iter1\n",
      "    + model_training_30_09_17_39_31\n",
      "        + iter1\n",
      "            + checkpoint.pt\n",
      "            + eval_results_scatter.png\n",
      "            + run_parameters.json\n",
      "            + tf_gradient_attributions\n",
      "                + scored_edges.tsv\n",
      "                + tf_importance_matrix.csv\n",
      "            + tf_vocab.json\n",
      "            + tg_vocab.json\n",
      "            + training_log.csv\n",
      "        + tf_gradient_attributions\n",
      "    + peak_tmp.bed\n",
      "    + tss_tmp.bed\n"
     ]
    }
   ],
   "source": [
    "def tree(directory):\n",
    "    print(f\"+ {directory}\")\n",
    "    for path in sorted(directory.rglob(\"*\")):\n",
    "        depth = len(path.relative_to(directory).parts)\n",
    "        spacer = \"    \" * depth\n",
    "        print(f\"{spacer}+ {path.name}\")\n",
    "        \n",
    "tree(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
