{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532b70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "atac_pseudobulk_df\n",
      "                      E7.5_REP1.AAACAGCCAAACCCTA  E7.5_REP1.AAACAGCCAGGAACTG  \\\n",
      "peak_id                                                                        \n",
      "chr1:3142536-3143136                    0.093238                    0.000000   \n",
      "chr1:3553099-3553699                    0.035439                    0.199449   \n",
      "chr1:3584996-3585596                    0.000000                    0.000000   \n",
      "\n",
      "                      E7.5_REP1.AAACAGCCATCCTGAA  E7.5_REP1.AAACAGCCATGCTATG  \\\n",
      "peak_id                                                                        \n",
      "chr1:3142536-3143136                         0.0                    0.040567   \n",
      "chr1:3553099-3553699                         0.0                    0.360751   \n",
      "chr1:3584996-3585596                         0.0                    0.000000   \n",
      "\n",
      "                      E7.5_REP1.AAACATGCAATGAATG  ...  \\\n",
      "peak_id                                           ...   \n",
      "chr1:3142536-3143136                    0.000000  ...   \n",
      "chr1:3553099-3553699                    0.000000  ...   \n",
      "chr1:3584996-3585596                    0.028881  ...   \n",
      "\n",
      "                      E7.5_REP1.TTTGTTGGTCCTTCTC  E7.5_REP1.TTTGTTGGTCGCAAAC  \\\n",
      "peak_id                                                                        \n",
      "chr1:3142536-3143136                    0.000000                    0.030894   \n",
      "chr1:3553099-3553699                    0.054931                    0.000000   \n",
      "chr1:3584996-3585596                    0.000000                    0.000000   \n",
      "\n",
      "                      E7.5_REP1.TTTGTTGGTTAAGCCA  E7.5_REP1.TTTGTTGGTTAATGAC  \\\n",
      "peak_id                                                                        \n",
      "chr1:3142536-3143136                    0.093371                    0.000000   \n",
      "chr1:3553099-3553699                    0.000000                    0.032312   \n",
      "chr1:3584996-3585596                    0.047713                    0.032312   \n",
      "\n",
      "                      E7.5_REP1.TTTGTTGGTTACTTGC  \n",
      "peak_id                                           \n",
      "chr1:3142536-3143136                         0.0  \n",
      "chr1:3553099-3553699                         0.0  \n",
      "chr1:3584996-3585596                         0.0  \n",
      "\n",
      "[3 rows x 7411 columns]\n",
      "(57324, 7411)\n",
      "\n",
      "rna_pseudobulk_df\n",
      "               E7.5_REP1.AAACAGCCAAACCCTA  E7.5_REP1.AAACAGCCAGGAACTG  \\\n",
      "gene_name                                                               \n",
      "0610009E02RIK                    0.029195                    0.040145   \n",
      "0610010F05RIK                    0.280652                    0.323722   \n",
      "0610040J01RIK                    0.387565                    0.000000   \n",
      "\n",
      "               E7.5_REP1.AAACAGCCATCCTGAA  E7.5_REP1.AAACAGCCATGCTATG  \\\n",
      "gene_name                                                               \n",
      "0610009E02RIK                    0.000000                    0.100867   \n",
      "0610010F05RIK                    0.000010                    0.210703   \n",
      "0610040J01RIK                    0.085814                    0.100423   \n",
      "\n",
      "               E7.5_REP1.AAACATGCAATGAATG  ...  E7.5_REP1.TTTGTTGGTCCTTCTC  \\\n",
      "gene_name                                  ...                               \n",
      "0610009E02RIK                    0.232718  ...                    0.190455   \n",
      "0610010F05RIK                    0.153686  ...                    0.042369   \n",
      "0610040J01RIK                    0.099137  ...                    0.028068   \n",
      "\n",
      "               E7.5_REP1.TTTGTTGGTCGCAAAC  E7.5_REP1.TTTGTTGGTTAAGCCA  \\\n",
      "gene_name                                                               \n",
      "0610009E02RIK                    0.026778                    0.013361   \n",
      "0610010F05RIK                    0.284674                    0.187651   \n",
      "0610040J01RIK                    0.606975                    0.596446   \n",
      "\n",
      "               E7.5_REP1.TTTGTTGGTTAATGAC  E7.5_REP1.TTTGTTGGTTACTTGC  \n",
      "gene_name                                                              \n",
      "0610009E02RIK                    0.025250                    0.000000  \n",
      "0610010F05RIK                    0.268597                    0.325537  \n",
      "0610040J01RIK                    0.000000                    0.828459  \n",
      "\n",
      "[3 rows x 7411 columns]\n",
      "(2925, 7411)\n",
      "\n",
      "peak_to_gene_dist_df\n",
      "      peak_chrom  peak_start   peak_end                   peak_id gene_chrom  \\\n",
      "23686       chr5   146230649  146231249  chr5:146230649-146231249       chr5   \n",
      "22667       chr5   113772200  113772800  chr5:113772200-113772800       chr5   \n",
      "10279      chr15    76080271   76080871   chr15:76080271-76080871      chr15   \n",
      "\n",
      "       gene_start   gene_end     TG  tss_distance  tss_distance_score  \n",
      "23686   146231249  146231249   CDK8             0              1.0000  \n",
      "22667   113772800  113772800   ISCU             0              1.0000  \n",
      "10279    76080870   76080870  PUF60             1              0.9998  \n",
      "(31182, 10)\n",
      "\n",
      "sliding_window_df\n",
      "    TF               peak_id  sliding_window_score\n",
      "0  AHR  chr1:3142536-3143136                  -0.0\n",
      "1  AHR  chr1:3553099-3553699                  -0.0\n",
      "2  AHR  chr1:3584996-3585596                  -0.0\n",
      "(108105520, 3)\n",
      "\n",
      "bear_no_beeline_df\n",
      "       TF         TG\n",
      "0   TCEA3     PRSS29\n",
      "1   EOMES        SLY\n",
      "2  POU5F1  LOC677369\n",
      "(60150, 2)\n",
      "\n",
      "bear_no_beeline_or_chip_df\n",
      "      TF      TG\n",
      "0      T    AVIL\n",
      "1  TCEA3  PRSS29\n",
      "2  GATA3   ISG15\n",
      "(55280, 2)\n",
      "\n",
      "bear_no_chip_df\n",
      "       TF       TG\n",
      "0   KDM2A  TRMT10C\n",
      "1  POU5F1     ADSL\n",
      "2   TCEA3   PRSS29\n",
      "(938722, 2)\n",
      "\n",
      "beeline_no_bear_df\n",
      "      TF       TG\n",
      "0  NANOG     PFN3\n",
      "1   GLI1  CFAP206\n",
      "2  DPPA2    ROBO4\n",
      "(22325, 2)\n",
      "\n",
      "beeline_no_chip_df\n",
      "       TF       TG\n",
      "0   KDM2A  TRMT10C\n",
      "1  POU5F1     ADSL\n",
      "2   DDX21   SCAMP1\n",
      "(905637, 2)\n",
      "\n",
      "beeline_no_chip_or_bear_df\n",
      "      TF       TG\n",
      "0  NANOG     PFN3\n",
      "1   GLI1  CFAP206\n",
      "2  DPPA2    ROBO4\n",
      "(22195, 2)\n",
      "\n",
      "chip_no_beeline_df\n",
      "       TF      TG\n",
      "0  SAP130   HERC2\n",
      "1  HOXA11  LINGO1\n",
      "2   TBL1X    MEI4\n",
      "(699155, 2)\n",
      "\n",
      "chip_no_beeline_or_bear_df\n",
      "       TF      TG\n",
      "0  SAP130   HERC2\n",
      "1  HOXA11  LINGO1\n",
      "2   TBL1X    MEI4\n",
      "(694285, 2)\n",
      "\n",
      "chip_no_bear_df\n",
      "       TF      TG\n",
      "0  SAP130   HERC2\n",
      "1  HOXA11  LINGO1\n",
      "2   TBL1X    MEI4\n",
      "(694415, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "\n",
    "base = Path(\"data/clean_classifier_data\")\n",
    "\n",
    "# ----- FILE REGISTRY -----\n",
    "parquet_files = {\n",
    "    \"atac_pseudobulk_df\": base / \"clean_data_files/atac_pseudobulk_df.parquet\",\n",
    "    \"rna_pseudobulk_df\":  base / \"clean_data_files/rna_pseudobulk_df.parquet\",\n",
    "    \"peak_to_gene_dist_df\": base / \"clean_feature_files/peak_to_gene_dist_df.parquet\",\n",
    "    \"sliding_window_df\":    base / \"clean_feature_files/sliding_window_df.parquet\",\n",
    "}\n",
    "\n",
    "csv_files = {\n",
    "    \"bear_no_beeline_df\":              base / \"clean_ground_truth_files/bear_no_beeline_df.csv\",\n",
    "    \"bear_no_beeline_or_chip_df\":      base / \"clean_ground_truth_files/bear_no_beeline_or_chip_df.csv\",\n",
    "    \"bear_no_chip_df\":                 base / \"clean_ground_truth_files/bear_no_chip_df.csv\",\n",
    "    \"beeline_no_bear_df\":              base / \"clean_ground_truth_files/beeline_no_bear_df.csv\",\n",
    "    \"beeline_no_chip_df\":              base / \"clean_ground_truth_files/beeline_no_chip_df.csv\",\n",
    "    \"beeline_no_chip_or_bear_df\":      base / \"clean_ground_truth_files/beeline_no_chip_or_bear_df.csv\",\n",
    "    \"chip_no_beeline_df\":              base / \"clean_ground_truth_files/chip_no_beeline_df.csv\",\n",
    "    \"chip_no_beeline_or_bear_df\":      base / \"clean_ground_truth_files/chip_no_beeline_or_bear_df.csv\",\n",
    "    \"chip_no_bear_df\":                 base / \"clean_ground_truth_files/chip_no_bear_df.csv\",\n",
    "}\n",
    "\n",
    "# ----- LOAD DATA -----\n",
    "dfs = {}\n",
    "for name, path in parquet_files.items():\n",
    "    dfs[name] = pd.read_parquet(path)\n",
    "for name, path in csv_files.items():\n",
    "    dfs[name] = pd.read_csv(path)\n",
    "    \n",
    "\n",
    "\n",
    "# ----- LOG HEADS & SHAPES -----\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(df.head(3))\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing correlation matrices...\n",
      "\n",
      "[1/4] Computing gene accessibility...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408e84c4e75c4994a7a141256ecd3d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Gene accessibility:   0%|          | 0/15161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Computing TF-TG expression correlations...\n",
      "  Computing Pearson correlations (vectorized)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887d96aa604641e3b8fd2831ce402576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Storing Pearson:   0%|          | 0/2925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computing Spearman correlations (vectorized)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35883df8bd64c35afcf969e25a13537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Storing Spearman:   0%|          | 0/2925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Computing TF-accessibility correlations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5c03558c84fcf97a3f0a803329202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  TF-accessibility:   0%|          | 0/2925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Computing TG expression-accessibility correlations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077f3aa52ca47869bcb20dd5e72deb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Gene expr-accessibility:   0%|          | 0/15161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Correlation matrices computed!\n",
      "\n",
      "Merging sliding window with peak-to-gene distances...\n",
      "  Merged shape: (58809252, 6)\n",
      "Computing binding features...\n",
      "Computing binding features (vectorized)...\n",
      "  Phase 1: Basic aggregations...\n",
      "  Phase 2: Strong peaks (75th percentile)...\n",
      "  Phase 3: Distance features (numba-accelerated)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9406dcba455a4d28a95700733a1b5f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Distance features:   0%|          | 0/3289937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Phase 4: Entropy and Gini (numba-accelerated)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739676f3ce454f99b93463da2354566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "    Entropy/Gini:   0%|          | 0/3289937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Computed binding features for 3289937 TF-TG pairs\n",
      "\n",
      "  Computed binding features for 3289937 TF-TG pairs\n",
      "\n",
      "Adding expression features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7573cc7882764a63a949b3e3b8f6839a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Computing expression features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def safe_divide(a, b, fill_value=0.0):\n",
    "    \"\"\"Safe division with fill value for division by zero.\"\"\"\n",
    "    return np.divide(a, b, out=np.full_like(a, fill_value, dtype=float), where=b!=0)\n",
    "\n",
    "def entropy(x):\n",
    "    \"\"\"Calculate Shannon entropy of array x.\"\"\"\n",
    "    x = x[x > 0]  # Remove zeros\n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    p = x / np.sum(x)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def partial_correlation(x, y, z):\n",
    "    \"\"\"Calculate partial correlation between x and y controlling for z.\"\"\"\n",
    "    if len(x) < 3:\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Residuals after regressing out z\n",
    "        x_resid = x - np.mean(x)\n",
    "        y_resid = y - np.mean(y)\n",
    "        z_norm = (z - np.mean(z)) / (np.std(z) + 1e-10)\n",
    "        \n",
    "        x_resid = x_resid - np.dot(x_resid, z_norm) * z_norm / (np.dot(z_norm, z_norm) + 1e-10)\n",
    "        y_resid = y_resid - np.dot(y_resid, z_norm) * z_norm / (np.dot(z_norm, z_norm) + 1e-10)\n",
    "        \n",
    "        # Correlation of residuals\n",
    "        corr = np.corrcoef(x_resid, y_resid)[0, 1]\n",
    "        return corr if not np.isnan(corr) else 0.0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_gini(x):\n",
    "    \"\"\"Numba-compiled Gini coefficient.\"\"\"\n",
    "    if len(x) == 0 or np.sum(x) == 0:\n",
    "        return 0.0\n",
    "    sorted_x = np.sort(x)\n",
    "    n = len(x)\n",
    "    cumsum = np.cumsum(sorted_x)\n",
    "    return (2 * np.sum((n - np.arange(n)) * sorted_x)) / (n * cumsum[-1]) - (n + 1) / n\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_entropy(x):\n",
    "    \"\"\"Numba-compiled Shannon entropy.\"\"\"\n",
    "    x_pos = x[x > 0]\n",
    "    if len(x_pos) == 0:\n",
    "        return 0.0\n",
    "    p = x_pos / np.sum(x_pos)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_distance_features(scores, tss_scores, tss_distances):\n",
    "    \"\"\"Numba-compiled distance feature computation.\"\"\"\n",
    "    n = len(scores)\n",
    "    \n",
    "    # Initialize outputs\n",
    "    min_tss_dist = np.inf\n",
    "    mean_tss_dist = np.inf\n",
    "    closest_peak_binding = 0.0\n",
    "    peaks_1kb = 0\n",
    "    peaks_5kb = 0\n",
    "    peaks_10kb = 0\n",
    "    peaks_50kb = 0\n",
    "    \n",
    "    abs_distances = np.abs(tss_distances)\n",
    "    binding_mask = scores > 0\n",
    "    \n",
    "    if np.sum(binding_mask) > 0:\n",
    "        bound_distances = abs_distances[binding_mask]\n",
    "        bound_scores = scores[binding_mask]\n",
    "        \n",
    "        min_tss_dist = np.min(bound_distances)\n",
    "        closest_idx = np.argmin(bound_distances)\n",
    "        closest_peak_binding = bound_scores[closest_idx]\n",
    "        \n",
    "        # Weighted mean distance\n",
    "        mean_tss_dist = np.sum(bound_distances * bound_scores) / np.sum(bound_scores)\n",
    "        \n",
    "        # Binned counts\n",
    "        peaks_1kb = np.sum((abs_distances < 1000) & binding_mask)\n",
    "        peaks_5kb = np.sum((abs_distances < 5000) & binding_mask)\n",
    "        peaks_10kb = np.sum((abs_distances < 10000) & binding_mask)\n",
    "        peaks_50kb = np.sum((abs_distances < 50000) & binding_mask)\n",
    "    \n",
    "    return min_tss_dist, mean_tss_dist, closest_peak_binding, peaks_1kb, peaks_5kb, peaks_10kb, peaks_50kb\n",
    "\n",
    "# ============================================================================\n",
    "# PRECOMPUTE CORRELATION MATRICES (ONE-TIME COMPUTATION)\n",
    "# ============================================================================\n",
    "\n",
    "def precompute_correlation_matrices(rna_df, atac_df, peak_to_gene_df):\n",
    "    \"\"\"\n",
    "    Pre-compute correlation matrices to avoid redundant calculations.\n",
    "    Fully vectorized for maximum speed.\n",
    "    \"\"\"\n",
    "    print(\"Pre-computing correlation matrices...\")\n",
    "    \n",
    "    # Get unique genes and TFs\n",
    "    genes = peak_to_gene_df['TG'].unique()\n",
    "    tfs = rna_df.index.tolist()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. GENE ACCESSIBILITY (sum of peaks per gene)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[1/4] Computing gene accessibility...\")\n",
    "    gene_to_peaks = peak_to_gene_df.groupby('TG')['peak_id'].apply(list).to_dict()\n",
    "    \n",
    "    gene_accessibility_dict = {}\n",
    "    for gene in tqdm(genes, desc=\"  Gene accessibility\"):\n",
    "        if gene in gene_to_peaks:\n",
    "            peaks = gene_to_peaks[gene]\n",
    "            valid_peaks = [p for p in peaks if p in atac_df.index]\n",
    "            if valid_peaks:\n",
    "                gene_accessibility_dict[gene] = atac_df.loc[valid_peaks].sum(axis=0).values\n",
    "            else:\n",
    "                gene_accessibility_dict[gene] = np.zeros(atac_df.shape[1])\n",
    "        else:\n",
    "            gene_accessibility_dict[gene] = np.zeros(atac_df.shape[1])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. TF-TG EXPRESSION CORRELATIONS (Vectorized)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/4] Computing TF-TG expression correlations...\")\n",
    "    \n",
    "    # Get subset of RNA matrix for genes of interest\n",
    "    genes_in_rna = [g for g in genes if g in rna_df.index]\n",
    "    tfs_in_rna = [tf for tf in tfs if tf in rna_df.index]\n",
    "    \n",
    "    # Create index mappings for fast lookup\n",
    "    rna_gene_to_idx = {gene: idx for idx, gene in enumerate(rna_df.index)}\n",
    "    \n",
    "    # Extract TF and gene expression matrices\n",
    "    tf_indices = [rna_gene_to_idx[tf] for tf in tfs_in_rna]\n",
    "    gene_indices = [rna_gene_to_idx[g] for g in genes_in_rna]\n",
    "    \n",
    "    tf_matrix = rna_df.values[tf_indices, :]  # (n_tfs, n_cells)\n",
    "    gene_matrix = rna_df.values[gene_indices, :]  # (n_genes, n_cells)\n",
    "    \n",
    "    # Compute Pearson correlation matrix all at once\n",
    "    print(\"  Computing Pearson correlations (vectorized)...\")\n",
    "    # Pearson: use numpy corrcoef on stacked arrays, then extract submatrix\n",
    "    # More memory efficient: compute in chunks if needed\n",
    "    \n",
    "    tf_tg_corr_pearson = {}\n",
    "    tf_tg_corr_spearman = {}\n",
    "    \n",
    "    # Use pandas for efficient correlation computation\n",
    "    tf_df = pd.DataFrame(tf_matrix, index=tfs_in_rna)\n",
    "    gene_df = pd.DataFrame(gene_matrix, index=genes_in_rna)\n",
    "    \n",
    "    # Compute full correlation matrix (TFs x Genes)\n",
    "    # This is much faster than nested loops\n",
    "    pearson_corr_matrix = np.corrcoef(tf_matrix, gene_matrix)[:len(tfs_in_rna), len(tfs_in_rna):]\n",
    "    \n",
    "    # Store in dictionary\n",
    "    for i, tf in enumerate(tqdm(tfs_in_rna, desc=\"  Storing Pearson\")):\n",
    "        for j, gene in enumerate(genes_in_rna):\n",
    "            corr_val = pearson_corr_matrix[i, j]\n",
    "            tf_tg_corr_pearson[(tf, gene)] = corr_val if not np.isnan(corr_val) else 0.0\n",
    "    \n",
    "    # Compute Spearman (on ranks) - this is slower, so we can do in batches\n",
    "    print(\"  Computing Spearman correlations (vectorized)...\")\n",
    "    \n",
    "    # Rank transform the matrices\n",
    "    from scipy.stats import rankdata\n",
    "    tf_ranks = np.apply_along_axis(rankdata, 1, tf_matrix)\n",
    "    gene_ranks = np.apply_along_axis(rankdata, 1, gene_matrix)\n",
    "    \n",
    "    # Compute correlation on ranks (Spearman)\n",
    "    spearman_corr_matrix = np.corrcoef(tf_ranks, gene_ranks)[:len(tfs_in_rna), len(tfs_in_rna):]\n",
    "    \n",
    "    for i, tf in enumerate(tqdm(tfs_in_rna, desc=\"  Storing Spearman\")):\n",
    "        for j, gene in enumerate(genes_in_rna):\n",
    "            corr_val = spearman_corr_matrix[i, j]\n",
    "            tf_tg_corr_spearman[(tf, gene)] = corr_val if not np.isnan(corr_val) else 0.0\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. TF-ACCESSIBILITY CORRELATIONS (Vectorized)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/4] Computing TF-accessibility correlations...\")\n",
    "    \n",
    "    # Create gene accessibility matrix\n",
    "    gene_acc_matrix = np.array([gene_accessibility_dict[g] for g in genes])  # (n_genes, n_cells)\n",
    "    \n",
    "    # Compute correlation between TFs and gene accessibility\n",
    "    tf_atac_corr_matrix = np.corrcoef(tf_matrix, gene_acc_matrix)[:len(tfs_in_rna), len(tfs_in_rna):]\n",
    "    \n",
    "    tf_atac_corr = {}\n",
    "    for i, tf in enumerate(tqdm(tfs_in_rna, desc=\"  TF-accessibility\")):\n",
    "        for j, gene in enumerate(genes):\n",
    "            corr_val = tf_atac_corr_matrix[i, j]\n",
    "            tf_atac_corr[(tf, gene)] = corr_val if not np.isnan(corr_val) else 0.0\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. TG EXPRESSION-ACCESSIBILITY CORRELATIONS (Vectorized)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/4] Computing TG expression-accessibility correlations...\")\n",
    "    \n",
    "    atac_rna_corr = {}\n",
    "    \n",
    "    # For each gene, compute correlation between its expression and accessibility\n",
    "    for i, gene in enumerate(tqdm(genes, desc=\"  Gene expr-accessibility\")):\n",
    "        if gene in genes_in_rna:\n",
    "            gene_idx = gene_indices[genes_in_rna.index(gene)]\n",
    "            tg_expr = rna_df.values[gene_idx, :]\n",
    "            gene_acc = gene_accessibility_dict[gene]\n",
    "            \n",
    "            corr = np.corrcoef(tg_expr, gene_acc)[0, 1]\n",
    "            atac_rna_corr[gene] = corr if not np.isnan(corr) else 0.0\n",
    "        else:\n",
    "            atac_rna_corr[gene] = 0.0\n",
    "    \n",
    "    print(\"\\n✓ Correlation matrices computed!\\n\")\n",
    "    \n",
    "    return {\n",
    "        'tf_tg_corr_pearson': tf_tg_corr_pearson,\n",
    "        'tf_tg_corr_spearman': tf_tg_corr_spearman,\n",
    "        'tf_atac_corr': tf_atac_corr,\n",
    "        'atac_rna_corr': atac_rna_corr,\n",
    "        'gene_accessibility_dict': gene_accessibility_dict\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FEATURE COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_binding_features_fast(tf_tg_binding):\n",
    "    \"\"\"\n",
    "    Fast vectorized computation of binding features.\n",
    "    Uses groupby with built-in aggregations + custom numba functions.\n",
    "    \"\"\"\n",
    "    print(\"Computing binding features (vectorized)...\")\n",
    "    \n",
    "    # Pre-compute weighted scores\n",
    "    tf_tg_binding['weighted_score'] = (\n",
    "        tf_tg_binding['sliding_window_score'] * tf_tg_binding['tss_distance_score']\n",
    "    )\n",
    "    tf_tg_binding['abs_tss_distance'] = np.abs(tf_tg_binding['tss_distance'])\n",
    "    tf_tg_binding['has_binding'] = (tf_tg_binding['sliding_window_score'] > 0).astype(int)\n",
    "    \n",
    "    # Proximity score (exponential decay)\n",
    "    tf_tg_binding['proximity_score'] = (\n",
    "        tf_tg_binding['sliding_window_score'] * \n",
    "        np.exp(-tf_tg_binding['abs_tss_distance'] / 1000)\n",
    "    )\n",
    "    \n",
    "    # Binned distance indicators\n",
    "    tf_tg_binding['within_1kb'] = (\n",
    "        (tf_tg_binding['abs_tss_distance'] < 1000) & \n",
    "        (tf_tg_binding['sliding_window_score'] > 0)\n",
    "    ).astype(int)\n",
    "    tf_tg_binding['within_5kb'] = (\n",
    "        (tf_tg_binding['abs_tss_distance'] < 5000) & \n",
    "        (tf_tg_binding['sliding_window_score'] > 0)\n",
    "    ).astype(int)\n",
    "    tf_tg_binding['within_10kb'] = (\n",
    "        (tf_tg_binding['abs_tss_distance'] < 10000) & \n",
    "        (tf_tg_binding['sliding_window_score'] > 0)\n",
    "    ).astype(int)\n",
    "    tf_tg_binding['within_50kb'] = (\n",
    "        (tf_tg_binding['abs_tss_distance'] < 50000) & \n",
    "        (tf_tg_binding['sliding_window_score'] > 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FAST AGGREGATION (built-in functions)\n",
    "    # ========================================================================\n",
    "    print(\"  Phase 1: Basic aggregations...\")\n",
    "    \n",
    "    agg_dict = {\n",
    "        'sliding_window_score': [\n",
    "            ('max_binding_score', 'max'),\n",
    "            ('mean_binding_score', 'mean'),\n",
    "            ('median_binding_score', 'median'),\n",
    "            ('std_binding_score', 'std'),\n",
    "            ('sum_binding_score', 'sum'),\n",
    "        ],\n",
    "        'weighted_score': [\n",
    "            ('distance_weighted_binding', 'sum'),\n",
    "            ('max_weighted_binding', 'max'),\n",
    "        ],\n",
    "        'has_binding': [\n",
    "            ('n_peaks_with_binding', 'sum'),\n",
    "        ],\n",
    "        'proximity_score': [\n",
    "            ('proximity_binding_score', 'sum'),\n",
    "        ],\n",
    "        'within_1kb': [('peaks_within_1kb', 'sum')],\n",
    "        'within_5kb': [('peaks_within_5kb', 'sum')],\n",
    "        'within_10kb': [('peaks_within_10kb', 'sum')],\n",
    "        'within_50kb': [('peaks_within_50kb', 'sum')],\n",
    "    }\n",
    "    \n",
    "    # This is MUCH faster than apply()\n",
    "    features = tf_tg_binding.groupby(['TF', 'TG'], observed=True).agg(**{\n",
    "        name: (col, func) for col, funcs in agg_dict.items() for name, func in funcs\n",
    "    })\n",
    "    \n",
    "    # Add fraction peaks bound\n",
    "    peak_counts = tf_tg_binding.groupby(['TF', 'TG'], observed=True).size()\n",
    "    features['fraction_peaks_bound'] = features['n_peaks_with_binding'] / peak_counts\n",
    "    \n",
    "    # Compute 75th percentile threshold and strong peaks count\n",
    "    print(\"  Phase 2: Strong peaks (75th percentile)...\")\n",
    "    percentile_75 = tf_tg_binding.groupby(['TF', 'TG'], observed=True)['sliding_window_score'].quantile(0.75)\n",
    "    \n",
    "    # Count peaks above 75th percentile\n",
    "    tf_tg_binding_with_p75 = tf_tg_binding.merge(\n",
    "        percentile_75.rename('p75'),\n",
    "        left_on=['TF', 'TG'],\n",
    "        right_index=True\n",
    "    )\n",
    "    tf_tg_binding_with_p75['is_strong'] = (\n",
    "        tf_tg_binding_with_p75['sliding_window_score'] > tf_tg_binding_with_p75['p75']\n",
    "    ).astype(int)\n",
    "    \n",
    "    n_strong_peaks = tf_tg_binding_with_p75.groupby(['TF', 'TG'], observed=True)['is_strong'].sum()\n",
    "    features['n_strong_peaks'] = n_strong_peaks\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DISTANCE FEATURES (using numba for speed)\n",
    "    # ========================================================================\n",
    "    print(\"  Phase 3: Distance features (numba-accelerated)...\")\n",
    "    \n",
    "    # Group data for numba processing\n",
    "    grouped = tf_tg_binding.groupby(['TF', 'TG'], observed=True)\n",
    "    \n",
    "    distance_results = []\n",
    "    for (tf, tg), group in tqdm(grouped, desc=\"    Distance features\"):\n",
    "        scores = group['sliding_window_score'].values\n",
    "        tss_scores = group['tss_distance_score'].values\n",
    "        tss_distances = group['tss_distance'].values\n",
    "        \n",
    "        min_dist, mean_dist, closest_binding, p1kb, p5kb, p10kb, p50kb = compute_distance_features(\n",
    "            scores, tss_scores, tss_distances\n",
    "        )\n",
    "        \n",
    "        distance_results.append({\n",
    "            'TF': tf,\n",
    "            'TG': tg,\n",
    "            'min_tss_distance': min_dist,\n",
    "            'mean_tss_distance': mean_dist,\n",
    "            'closest_peak_binding': closest_binding,\n",
    "            'peaks_within_1kb_check': p1kb,\n",
    "            'peaks_within_5kb_check': p5kb,\n",
    "            'peaks_within_10kb_check': p10kb,\n",
    "            'peaks_within_50kb_check': p50kb,\n",
    "        })\n",
    "    \n",
    "    distance_df = pd.DataFrame(distance_results).set_index(['TF', 'TG'])\n",
    "    \n",
    "    # Merge distance features\n",
    "    features = features.join(distance_df)\n",
    "    \n",
    "    # Use the numba-computed binned counts (they should match the vectorized ones, but numba is more accurate)\n",
    "    features['peaks_within_1kb'] = features['peaks_within_1kb_check']\n",
    "    features['peaks_within_5kb'] = features['peaks_within_5kb_check']\n",
    "    features['peaks_within_10kb'] = features['peaks_within_10kb_check']\n",
    "    features['peaks_within_50kb'] = features['peaks_within_50kb_check']\n",
    "    features.drop(columns=['peaks_within_1kb_check', 'peaks_within_5kb_check', \n",
    "                           'peaks_within_10kb_check', 'peaks_within_50kb_check'], inplace=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ENTROPY AND GINI (using numba)\n",
    "    # ========================================================================\n",
    "    print(\"  Phase 4: Entropy and Gini (numba-accelerated)...\")\n",
    "    \n",
    "    entropy_results = []\n",
    "    for (tf, tg), group in tqdm(grouped, desc=\"    Entropy/Gini\"):\n",
    "        scores = group['sliding_window_score'].values\n",
    "        \n",
    "        entropy_val = fast_entropy(scores)\n",
    "        gini_val = fast_gini(scores)\n",
    "        \n",
    "        entropy_results.append({\n",
    "            'TF': tf,\n",
    "            'TG': tg,\n",
    "            'binding_entropy': entropy_val,\n",
    "            'binding_gini': gini_val,\n",
    "        })\n",
    "    \n",
    "    entropy_df = pd.DataFrame(entropy_results).set_index(['TF', 'TG'])\n",
    "    features = features.join(entropy_df)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # NEGATIVE EVIDENCE\n",
    "    # ========================================================================\n",
    "    features['no_binding_near_tss'] = (features['peaks_within_10kb'] == 0).astype(int)\n",
    "    \n",
    "    # Reset index\n",
    "    features = features.reset_index()\n",
    "    \n",
    "    print(f\"✓ Computed binding features for {len(features)} TF-TG pairs\\n\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def add_expression_features(features_df, rna_df, precomputed):\n",
    "    \"\"\"Add expression-based features.\"\"\"\n",
    "    print(\"Adding expression features...\")\n",
    "    \n",
    "    rna_genes = rna_df.index.tolist()\n",
    "    rna_matrix = rna_df.values\n",
    "    \n",
    "    # Vectorized expression features\n",
    "    tf_list = features_df['TF'].values\n",
    "    tg_list = features_df['TG'].values\n",
    "    \n",
    "    # Initialize arrays\n",
    "    n_pairs = len(features_df)\n",
    "    tf_mean_expr = np.zeros(n_pairs)\n",
    "    tf_std_expr = np.zeros(n_pairs)\n",
    "    tf_cv = np.zeros(n_pairs)\n",
    "    tf_detection_rate = np.zeros(n_pairs)\n",
    "    \n",
    "    tg_mean_expr = np.zeros(n_pairs)\n",
    "    tg_std_expr = np.zeros(n_pairs)\n",
    "    tg_cv = np.zeros(n_pairs)\n",
    "    tg_detection_rate = np.zeros(n_pairs)\n",
    "    \n",
    "    tf_tg_expr_ratio = np.zeros(n_pairs)\n",
    "    codetection_rate = np.zeros(n_pairs)\n",
    "    \n",
    "    tf_tg_pearson_corr = np.zeros(n_pairs)\n",
    "    tf_tg_spearman_corr = np.zeros(n_pairs)\n",
    "    \n",
    "    # Context-aware features\n",
    "    tf_active_when_tg_active = np.zeros(n_pairs)\n",
    "    tg_expr_in_top_tf_quartile = np.zeros(n_pairs)\n",
    "    tg_expr_in_bottom_tf_quartile = np.zeros(n_pairs)\n",
    "    differential_tg_expr = np.zeros(n_pairs)\n",
    "    \n",
    "    tf_not_expressed = np.zeros(n_pairs)\n",
    "    anticorrelation_flag = np.zeros(n_pairs)\n",
    "    \n",
    "    for i, (tf, tg) in enumerate(tqdm(zip(tf_list, tg_list), desc=\"  Computing expression features\")):\n",
    "        # Get expression data\n",
    "        if tf in rna_genes:\n",
    "            tf_idx = rna_genes.index(tf)\n",
    "            tf_expr = rna_matrix[tf_idx]\n",
    "            \n",
    "            tf_mean_expr[i] = np.mean(tf_expr)\n",
    "            tf_std_expr[i] = np.std(tf_expr)\n",
    "            tf_cv[i] = tf_std_expr[i] / (tf_mean_expr[i] + 1e-10)\n",
    "            tf_detection_rate[i] = np.mean(tf_expr > 0)\n",
    "            \n",
    "            # Thresholds for context features\n",
    "            tf_threshold = np.percentile(tf_expr, 25) if tf_mean_expr[i] > 0 else 0\n",
    "            tf_75 = np.percentile(tf_expr, 75)\n",
    "            tf_25 = np.percentile(tf_expr, 25)\n",
    "            \n",
    "            tf_not_expressed[i] = 1 if tf_mean_expr[i] < tf_threshold else 0\n",
    "        else:\n",
    "            tf_expr = None\n",
    "            tf_not_expressed[i] = 1\n",
    "        \n",
    "        if tg in rna_genes:\n",
    "            tg_idx = rna_genes.index(tg)\n",
    "            tg_expr_arr = rna_matrix[tg_idx]\n",
    "            \n",
    "            tg_mean_expr[i] = np.mean(tg_expr_arr)\n",
    "            tg_std_expr[i] = np.std(tg_expr_arr)\n",
    "            tg_cv[i] = tg_std_expr[i] / (tg_mean_expr[i] + 1e-10)\n",
    "            tg_detection_rate[i] = np.mean(tg_expr_arr > 0)\n",
    "            \n",
    "            # TG threshold for context features\n",
    "            tg_threshold = np.percentile(tg_expr_arr, 25) if tg_mean_expr[i] > 0 else 0\n",
    "        else:\n",
    "            tg_expr_arr = None\n",
    "        \n",
    "        # TF-TG relationship features\n",
    "        if tf_expr is not None and tg_expr_arr is not None:\n",
    "            tf_tg_expr_ratio[i] = tf_mean_expr[i] / (tg_mean_expr[i] + 1e-10)\n",
    "            codetection_rate[i] = np.mean((tf_expr > 0) & (tg_expr_arr > 0))\n",
    "            \n",
    "            # Get precomputed correlations\n",
    "            tf_tg_pearson_corr[i] = precomputed['tf_tg_corr_pearson'].get((tf, tg), 0.0)\n",
    "            tf_tg_spearman_corr[i] = precomputed['tf_tg_corr_spearman'].get((tf, tg), 0.0)\n",
    "            \n",
    "            # Context-aware features\n",
    "            tg_active_mask = tg_expr_arr > tg_threshold\n",
    "            if np.any(tg_active_mask):\n",
    "                tf_active_when_tg_active[i] = np.mean(tf_expr[tg_active_mask])\n",
    "            \n",
    "            tf_top_mask = tf_expr > tf_75\n",
    "            tf_bottom_mask = tf_expr < tf_25\n",
    "            \n",
    "            if np.any(tf_top_mask):\n",
    "                tg_expr_in_top_tf_quartile[i] = np.mean(tg_expr_arr[tf_top_mask])\n",
    "            if np.any(tf_bottom_mask):\n",
    "                tg_expr_in_bottom_tf_quartile[i] = np.mean(tg_expr_arr[tf_bottom_mask])\n",
    "            \n",
    "            differential_tg_expr[i] = tg_expr_in_top_tf_quartile[i] - tg_expr_in_bottom_tf_quartile[i]\n",
    "            \n",
    "            # Negative evidence\n",
    "            anticorrelation_flag[i] = 1 if tf_tg_pearson_corr[i] < -0.3 else 0\n",
    "    \n",
    "    # Add to dataframe\n",
    "    features_df['tf_mean_expr'] = tf_mean_expr\n",
    "    features_df['tf_std_expr'] = tf_std_expr\n",
    "    features_df['tf_cv'] = tf_cv\n",
    "    features_df['tf_detection_rate'] = tf_detection_rate\n",
    "    \n",
    "    features_df['tg_mean_expr'] = tg_mean_expr\n",
    "    features_df['tg_std_expr'] = tg_std_expr\n",
    "    features_df['tg_cv'] = tg_cv\n",
    "    features_df['tg_detection_rate'] = tg_detection_rate\n",
    "    \n",
    "    features_df['tf_tg_expr_ratio'] = tf_tg_expr_ratio\n",
    "    features_df['tf_tg_pearson_corr'] = tf_tg_pearson_corr\n",
    "    features_df['tf_tg_spearman_corr'] = tf_tg_spearman_corr\n",
    "    features_df['codetection_rate'] = codetection_rate\n",
    "    \n",
    "    features_df['tf_active_when_tg_active'] = tf_active_when_tg_active\n",
    "    features_df['tg_expr_in_top_tf_quartile'] = tg_expr_in_top_tf_quartile\n",
    "    features_df['tg_expr_in_bottom_tf_quartile'] = tg_expr_in_bottom_tf_quartile\n",
    "    features_df['differential_tg_expr'] = differential_tg_expr\n",
    "    \n",
    "    features_df['tf_not_expressed'] = tf_not_expressed\n",
    "    features_df['anticorrelation_flag'] = anticorrelation_flag\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def add_accessibility_features(features_df, rna_df, atac_df, precomputed):\n",
    "    \"\"\"Add accessibility-based features.\"\"\"\n",
    "    print(\"Adding accessibility features...\")\n",
    "    \n",
    "    rna_genes = rna_df.index.tolist()\n",
    "    rna_matrix = rna_df.values\n",
    "    \n",
    "    tf_list = features_df['TF'].values\n",
    "    tg_list = features_df['TG'].values\n",
    "    \n",
    "    n_pairs = len(features_df)\n",
    "    \n",
    "    # Initialize arrays\n",
    "    gene_accessibility_mean = np.zeros(n_pairs)\n",
    "    gene_accessibility_std = np.zeros(n_pairs)\n",
    "    gene_accessibility_cv = np.zeros(n_pairs)\n",
    "    atac_variability_ratio = np.zeros(n_pairs)\n",
    "    \n",
    "    atac_rna_corr = np.zeros(n_pairs)\n",
    "    tf_atac_corr = np.zeros(n_pairs)\n",
    "    tf_expr_tg_atac_corr = np.zeros(n_pairs)\n",
    "    \n",
    "    atac_when_tf_expressed = np.zeros(n_pairs)\n",
    "    partial_corr_tg_atac_ctrl_tf = np.zeros(n_pairs)\n",
    "    \n",
    "    for i, (tf, tg) in enumerate(tqdm(zip(tf_list, tg_list), desc=\"  Computing accessibility features\")):\n",
    "        # Get gene accessibility\n",
    "        if tg in precomputed['gene_accessibility_dict']:\n",
    "            gene_acc = precomputed['gene_accessibility_dict'][tg]\n",
    "            \n",
    "            gene_accessibility_mean[i] = np.mean(gene_acc)\n",
    "            gene_accessibility_std[i] = np.std(gene_acc)\n",
    "            gene_accessibility_cv[i] = gene_accessibility_std[i] / (gene_accessibility_mean[i] + 1e-10)\n",
    "            atac_variability_ratio[i] = gene_accessibility_std[i] / (gene_accessibility_mean[i] + 1e-10)\n",
    "            \n",
    "            # Get precomputed correlations\n",
    "            atac_rna_corr[i] = precomputed['atac_rna_corr'].get(tg, 0.0)\n",
    "            tf_atac_corr[i] = precomputed['tf_atac_corr'].get((tf, tg), 0.0)\n",
    "            tf_expr_tg_atac_corr[i] = tf_atac_corr[i]  # Same thing\n",
    "            \n",
    "            # Context-aware: accessibility when TF is expressed\n",
    "            if tf in rna_genes:\n",
    "                tf_idx = rna_genes.index(tf)\n",
    "                tf_expr = rna_matrix[tf_idx]\n",
    "                \n",
    "                tf_threshold = np.percentile(tf_expr, 25) if np.mean(tf_expr) > 0 else 0\n",
    "                tf_expressed_mask = tf_expr > tf_threshold\n",
    "                \n",
    "                if np.any(tf_expressed_mask):\n",
    "                    atac_when_tf_expressed[i] = np.mean(gene_acc[tf_expressed_mask])\n",
    "                \n",
    "                # Partial correlation\n",
    "                if tg in rna_genes:\n",
    "                    tg_idx = rna_genes.index(tg)\n",
    "                    tg_expr = rna_matrix[tg_idx]\n",
    "                    \n",
    "                    partial_corr_tg_atac_ctrl_tf[i] = partial_correlation(tg_expr, gene_acc, tf_expr)\n",
    "    \n",
    "    # Add to dataframe\n",
    "    features_df['gene_accessibility_mean'] = gene_accessibility_mean\n",
    "    features_df['gene_accessibility_std'] = gene_accessibility_std\n",
    "    features_df['gene_accessibility_cv'] = gene_accessibility_cv\n",
    "    features_df['atac_variability_ratio'] = atac_variability_ratio\n",
    "    \n",
    "    features_df['atac_rna_corr'] = atac_rna_corr\n",
    "    features_df['tf_atac_corr'] = tf_atac_corr\n",
    "    features_df['tf_expr_tg_atac_corr'] = tf_expr_tg_atac_corr\n",
    "    \n",
    "    features_df['atac_when_tf_expressed'] = atac_when_tf_expressed\n",
    "    features_df['partial_corr_tg_atac_ctrl_tf'] = partial_corr_tg_atac_ctrl_tf\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def add_interaction_features(features_df):\n",
    "    \"\"\"Add interaction and composite features.\"\"\"\n",
    "    print(\"Adding interaction features...\")\n",
    "    \n",
    "    # Interaction terms\n",
    "    features_df['binding_x_tf_expr'] = features_df['max_binding_score'] * features_df['tf_mean_expr']\n",
    "    features_df['binding_x_corr'] = features_df['max_binding_score'] * features_df['tf_tg_pearson_corr']\n",
    "    features_df['binding_x_distance'] = features_df['max_binding_score'] * safe_divide(\n",
    "        1.0, features_df['min_tss_distance'].replace([np.inf, -np.inf], 1e6), fill_value=0.0\n",
    "    )\n",
    "    \n",
    "    # Regulatory potential score (composite)\n",
    "    # Use distance_weighted_binding as a proxy for max_weighted_binding * tss_distance_score\n",
    "    features_df['regulatory_potential'] = (\n",
    "        features_df['max_weighted_binding'] * \n",
    "        features_df['tf_mean_expr'] * \n",
    "        np.abs(features_df['tf_tg_pearson_corr'])\n",
    "    )\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def compute_all_features(sliding_window_df, peak_to_gene_dist_df, rna_df, atac_df, \n",
    "                          tf_tg_pairs=None, batch_size=None):\n",
    "    \"\"\"\n",
    "    Main function to compute all features for TF-TG pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sliding_window_df : DataFrame with columns ['TF', 'peak_id', 'sliding_window_score']\n",
    "    peak_to_gene_dist_df : DataFrame with peak-to-gene mappings\n",
    "    rna_df : DataFrame with RNA expression (genes x cells)\n",
    "    atac_df : DataFrame with ATAC accessibility (peaks x cells)\n",
    "    tf_tg_pairs : Optional list of (TF, TG) tuples to compute features for.\n",
    "                  If None, compute for all possible pairs.\n",
    "    batch_size : If provided, process in batches (useful for very large datasets)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features_df : DataFrame with all computed features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Pre-compute correlations (one-time cost)\n",
    "    precomputed = precompute_correlation_matrices(rna_df, atac_df, peak_to_gene_dist_df)\n",
    "    \n",
    "    # Step 2: Merge sliding window with peak-to-gene links\n",
    "    print(\"Merging sliding window with peak-to-gene distances...\")\n",
    "    tf_tg_binding = sliding_window_df.merge(\n",
    "        peak_to_gene_dist_df[['peak_id', 'TG', 'tss_distance', 'tss_distance_score']], \n",
    "        on='peak_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    print(f\"  Merged shape: {tf_tg_binding.shape}\")\n",
    "    \n",
    "    # Step 3: Compute binding features via groupby\n",
    "    print(\"Computing binding features...\")\n",
    "    binding_features = compute_binding_features_fast(tf_tg_binding)\n",
    "    \n",
    "    print(f\"  Computed binding features for {len(binding_features)} TF-TG pairs\\n\")\n",
    "    \n",
    "    # Step 4: Add expression features\n",
    "    features_df = add_expression_features(binding_features, rna_df, precomputed)\n",
    "    \n",
    "    # Step 5: Add accessibility features\n",
    "    features_df = add_accessibility_features(features_df, rna_df, atac_df, precomputed)\n",
    "    \n",
    "    # Step 6: Add interaction features\n",
    "    features_df = add_interaction_features(features_df)\n",
    "    \n",
    "    # Handle any remaining NaN or inf values\n",
    "    print(\"Cleaning up features...\")\n",
    "    features_df = features_df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Fill NaN with 0 for most features (or could use median/mean)\n",
    "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "    features_df[numeric_cols] = features_df[numeric_cols].fillna(0)\n",
    "    \n",
    "    print(f\"\\n✓ Feature computation complete!\")\n",
    "    print(f\"  Final shape: {features_df.shape}\")\n",
    "    print(f\"  Features: {features_df.shape[1] - 2} (excluding TF and TG columns)\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "sliding_window_df = dfs[\"sliding_window_df\"]\n",
    "peak_to_gene_dist_df = dfs[\"peak_to_gene_dist_df\"]\n",
    "rna_pseudobulk_df = dfs[\"rna_pseudobulk_df\"]\n",
    "atac_pseudobulk_df = dfs[\"atac_pseudobulk_df\"]\n",
    "\n",
    "features_df = compute_all_features(\n",
    "    sliding_window_df=sliding_window_df,\n",
    "    peak_to_gene_dist_df=peak_to_gene_dist_df,\n",
    "    rna_df=rna_pseudobulk_df,\n",
    "    atac_df=atac_pseudobulk_df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
